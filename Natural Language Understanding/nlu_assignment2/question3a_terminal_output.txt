Retained 2000 words from 38444 (84.00% of all tokens)


Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 5
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 7.89394894971

epoch 1, learning rate 0.5000	instance 1000	epoch done in 55.50 seconds	new loss: 7.95807830116
epoch 2, learning rate 0.4167	instance 1000	epoch done in 60.83 seconds	new loss: 5.6897471752
epoch 3, learning rate 0.3571	instance 1000	epoch done in 55.22 seconds	new loss: 5.62001068372
epoch 4, learning rate 0.3125	instance 1000	epoch done in 49.67 seconds	new loss: 5.34637162202
epoch 5, learning rate 0.2778	instance 1000	epoch done in 47.36 seconds	new loss: 5.24111390336
epoch 6, learning rate 0.2500	instance 1000	epoch done in 50.67 seconds	new loss: 5.17128551672
epoch 7, learning rate 0.2273	instance 1000	epoch done in 44.92 seconds	new loss: 5.14369524036
epoch 8, learning rate 0.2083	instance 1000	epoch done in 45.13 seconds	new loss: 5.12311443798
epoch 9, learning rate 0.1923	instance 1000	epoch done in 44.75 seconds	new loss: 5.10458956892
epoch 10, learning rate 0.1786	instance 1000	epoch done in 44.94 seconds	new loss: 5.08968205528

training finished after reaching maximum of 10 epochs
best observed loss was 5.08968205528, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 5
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 7.75852803576

epoch 1, learning rate 0.1000	instance 1000	epoch done in 44.75 seconds	new loss: 5.89859763383
epoch 2, learning rate 0.0833	instance 1000	epoch done in 45.18 seconds	new loss: 5.6237594332
epoch 3, learning rate 0.0714	instance 1000	epoch done in 46.00 seconds	new loss: 5.51708011067
epoch 4, learning rate 0.0625	instance 1000	epoch done in 51.92 seconds	new loss: 5.45470914952
epoch 5, learning rate 0.0556	instance 1000	epoch done in 51.21 seconds	new loss: 5.40407443443
epoch 6, learning rate 0.0500	instance 1000	epoch done in 46.76 seconds	new loss: 5.36846806248
epoch 7, learning rate 0.0455	instance 1000	epoch done in 46.44 seconds	new loss: 5.33894085265
epoch 8, learning rate 0.0417	instance 1000	epoch done in 48.03 seconds	new loss: 5.31439632724
epoch 9, learning rate 0.0385	instance 1000	epoch done in 46.18 seconds	new loss: 5.29591667696
epoch 10, learning rate 0.0357	instance 1000	epoch done in 46.84 seconds	new loss: 5.2774124931

training finished after reaching maximum of 10 epochs
best observed loss was 5.2774124931, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 5
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 7.84969528589

epoch 1, learning rate 0.0500	instance 1000	epoch done in 45.90 seconds	new loss: 6.35269693379
epoch 2, learning rate 0.0417	instance 1000	epoch done in 46.95 seconds	new loss: 5.99104464193
epoch 3, learning rate 0.0357	instance 1000	epoch done in 46.01 seconds	new loss: 5.82337271387
epoch 4, learning rate 0.0312	instance 1000	epoch done in 46.83 seconds	new loss: 5.73218970213
epoch 5, learning rate 0.0278	instance 1000	epoch done in 47.84 seconds	new loss: 5.66899309866
epoch 6, learning rate 0.0250	instance 1000	epoch done in 45.98 seconds	new loss: 5.62394555558
epoch 7, learning rate 0.0227	instance 1000	epoch done in 46.83 seconds	new loss: 5.59022170892
epoch 8, learning rate 0.0208	instance 1000	epoch done in 46.39 seconds	new loss: 5.5631849764
epoch 9, learning rate 0.0192	instance 1000	epoch done in 46.51 seconds	new loss: 5.54056782585
epoch 10, learning rate 0.0179	instance 1000	epoch done in 47.03 seconds	new loss: 5.52080582657

training finished after reaching maximum of 10 epochs
best observed loss was 5.52080582657, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 2
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.00326122347

epoch 1, learning rate 0.5000	instance 1000	epoch done in 48.16 seconds	new loss: 5.93727854504
epoch 2, learning rate 0.4167	instance 1000	epoch done in 41.80 seconds	new loss: 5.55058962103
epoch 3, learning rate 0.3571	instance 1000	epoch done in 40.17 seconds	new loss: 5.4658432226
epoch 4, learning rate 0.3125	instance 1000	epoch done in 40.19 seconds	new loss: 5.22647658422
epoch 5, learning rate 0.2778	instance 1000	epoch done in 41.30 seconds	new loss: 5.30150525312
epoch 6, learning rate 0.2500	instance 1000	epoch done in 40.52 seconds	new loss: 5.12515886788
epoch 7, learning rate 0.2273	instance 1000	epoch done in 41.04 seconds	new loss: 5.11469092589
epoch 8, learning rate 0.2083	instance 1000	epoch done in 41.68 seconds	new loss: 5.08340067293
epoch 9, learning rate 0.1923	instance 1000	epoch done in 40.12 seconds	new loss: 5.07624065675
epoch 10, learning rate 0.1786	instance 1000	epoch done in 41.54 seconds	new loss: 5.0684226547

training finished after reaching maximum of 10 epochs
best observed loss was 5.0684226547, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 2
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 7.85386140846

epoch 1, learning rate 0.1000	instance 1000	epoch done in 41.42 seconds	new loss: 5.95737882894
epoch 2, learning rate 0.0833	instance 1000	epoch done in 41.14 seconds	new loss: 5.6289197048
epoch 3, learning rate 0.0714	instance 1000	epoch done in 40.78 seconds	new loss: 5.54926632142
epoch 4, learning rate 0.0625	instance 1000	epoch done in 42.07 seconds	new loss: 5.46589905432
epoch 5, learning rate 0.0556	instance 1000	epoch done in 38.93 seconds	new loss: 5.41319442386
epoch 6, learning rate 0.0500	instance 1000	epoch done in 38.84 seconds	new loss: 5.37370204947
epoch 7, learning rate 0.0455	instance 1000	epoch done in 41.39 seconds	new loss: 5.34549020882
epoch 8, learning rate 0.0417	instance 1000	epoch done in 45.74 seconds	new loss: 5.3228963688
epoch 9, learning rate 0.0385	instance 1000	epoch done in 44.43 seconds	new loss: 5.30333736238
epoch 10, learning rate 0.0357	instance 1000	epoch done in 42.88 seconds	new loss: 5.28492860926

training finished after reaching maximum of 10 epochs
best observed loss was 5.28492860926, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 2
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 7.68278666116

epoch 1, learning rate 0.0500	instance 1000	epoch done in 41.88 seconds	new loss: 6.2985348384
epoch 2, learning rate 0.0417	instance 1000	epoch done in 42.71 seconds	new loss: 5.89759234741
epoch 3, learning rate 0.0357	instance 1000	epoch done in 40.52 seconds	new loss: 5.75403026941
epoch 4, learning rate 0.0312	instance 1000	epoch done in 39.72 seconds	new loss: 5.67590707527
epoch 5, learning rate 0.0278	instance 1000	epoch done in 39.19 seconds	new loss: 5.6232011682
epoch 6, learning rate 0.0250	instance 1000	epoch done in 39.00 seconds	new loss: 5.58513613969
epoch 7, learning rate 0.0227	instance 1000	epoch done in 39.02 seconds	new loss: 5.55370505925
epoch 8, learning rate 0.0208	instance 1000	epoch done in 42.68 seconds	new loss: 5.52819389834
epoch 9, learning rate 0.0192	instance 1000	epoch done in 47.55 seconds	new loss: 5.50596871355
epoch 10, learning rate 0.0179	instance 1000	epoch done in 46.25 seconds	new loss: 5.48727256297

training finished after reaching maximum of 10 epochs
best observed loss was 5.48727256297, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.01793038639

epoch 1, learning rate 0.5000	instance 1000	epoch done in 36.26 seconds	new loss: 6.97122529865
epoch 2, learning rate 0.4167	instance 1000	epoch done in 32.70 seconds	new loss: 5.63657580191
epoch 3, learning rate 0.3571	instance 1000	epoch done in 31.46 seconds	new loss: 5.91074281875
epoch 4, learning rate 0.3125	instance 1000	epoch done in 31.31 seconds	new loss: 5.22497799733
epoch 5, learning rate 0.2778	instance 1000	epoch done in 31.38 seconds	new loss: 5.23584390218
epoch 6, learning rate 0.2500	instance 1000	epoch done in 31.26 seconds	new loss: 5.15444291545
epoch 7, learning rate 0.2273	instance 1000	epoch done in 31.59 seconds	new loss: 5.10624397674
epoch 8, learning rate 0.2083	instance 1000	epoch done in 31.22 seconds	new loss: 5.0999730556
epoch 9, learning rate 0.1923	instance 1000	epoch done in 31.19 seconds	new loss: 5.07773564283
epoch 10, learning rate 0.1786	instance 1000	epoch done in 31.34 seconds	new loss: 5.06070020875

training finished after reaching maximum of 10 epochs
best observed loss was 5.06070020875, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 7.89230827359

epoch 1, learning rate 0.1000	instance 1000	epoch done in 31.60 seconds	new loss: 5.94496170881
epoch 2, learning rate 0.0833	instance 1000	epoch done in 31.28 seconds	new loss: 5.66196987863
epoch 3, learning rate 0.0714	instance 1000	epoch done in 31.27 seconds	new loss: 5.54660532919
epoch 4, learning rate 0.0625	instance 1000	epoch done in 31.38 seconds	new loss: 5.47716435895
epoch 5, learning rate 0.0556	instance 1000	epoch done in 31.59 seconds	new loss: 5.42547407528
epoch 6, learning rate 0.0500	instance 1000	epoch done in 31.40 seconds	new loss: 5.38974821504
epoch 7, learning rate 0.0455	instance 1000	epoch done in 31.23 seconds	new loss: 5.35877426802
epoch 8, learning rate 0.0417	instance 1000	epoch done in 31.35 seconds	new loss: 5.33123273358
epoch 9, learning rate 0.0385	instance 1000	epoch done in 31.16 seconds	new loss: 5.31154498007
epoch 10, learning rate 0.0357	instance 1000	epoch done in 31.25 seconds	new loss: 5.29362523025

training finished after reaching maximum of 10 epochs
best observed loss was 5.29362523025, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 25
Steps for back propagation: 0
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 7.95577095477

epoch 1, learning rate 0.0500	instance 1000	epoch done in 31.17 seconds	new loss: 6.21032643696
epoch 2, learning rate 0.0417	instance 1000	epoch done in 31.33 seconds	new loss: 5.84123835459
epoch 3, learning rate 0.0357	instance 1000	epoch done in 31.12 seconds	new loss: 5.70596654484
epoch 4, learning rate 0.0312	instance 1000	epoch done in 31.19 seconds	new loss: 5.63659895618
epoch 5, learning rate 0.0278	instance 1000	epoch done in 31.11 seconds	new loss: 5.59271312611
epoch 6, learning rate 0.0250	instance 1000	epoch done in 31.14 seconds	new loss: 5.55971001779
epoch 7, learning rate 0.0227	instance 1000	epoch done in 30.98 seconds	new loss: 5.53146872039
epoch 8, learning rate 0.0208	instance 1000	epoch done in 31.35 seconds	new loss: 5.50833871608
epoch 9, learning rate 0.0192	instance 1000	epoch done in 30.99 seconds	new loss: 5.48917879865
epoch 10, learning rate 0.0179	instance 1000	epoch done in 31.19 seconds	new loss: 5.4711824036

training finished after reaching maximum of 10 epochs
best observed loss was 5.4711824036, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 5
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 7.80030403268

epoch 1, learning rate 0.5000	instance 1000	epoch done in 68.30 seconds	new loss: 7.97983796051
epoch 2, learning rate 0.4167	instance 1000	epoch done in 70.81 seconds	new loss: 6.39888444938
epoch 3, learning rate 0.3571	instance 1000	epoch done in 74.11 seconds	new loss: 5.41133613415
epoch 4, learning rate 0.3125	instance 1000	epoch done in 68.63 seconds	new loss: 5.30093442256
epoch 5, learning rate 0.2778	instance 1000	epoch done in 84.65 seconds	new loss: 5.22965419899
epoch 6, learning rate 0.2500	instance 1000	epoch done in 78.49 seconds	new loss: 5.17133374127
epoch 7, learning rate 0.2273	instance 1000	epoch done in 71.99 seconds	new loss: 5.12976477984
epoch 8, learning rate 0.2083	instance 1000	epoch done in 73.16 seconds	new loss: 5.10624432359
epoch 9, learning rate 0.1923	instance 1000	epoch done in 74.75 seconds	new loss: 5.08593867329
epoch 10, learning rate 0.1786	instance 1000	epoch done in 73.50 seconds	new loss: 5.06864062001

training finished after reaching maximum of 10 epochs
best observed loss was 5.06864062001, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 5
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 7.99863229401

epoch 1, learning rate 0.1000	instance 1000	epoch done in 73.08 seconds	new loss: 5.9368643672
epoch 2, learning rate 0.0833	instance 1000	epoch done in 72.66 seconds	new loss: 5.63259256604
epoch 3, learning rate 0.0714	instance 1000	epoch done in 72.33 seconds	new loss: 5.4808062376
epoch 4, learning rate 0.0625	instance 1000	epoch done in 72.77 seconds	new loss: 5.41871569842
epoch 5, learning rate 0.0556	instance 1000	epoch done in 72.12 seconds	new loss: 5.3522467003
epoch 6, learning rate 0.0500	instance 1000	epoch done in 72.78 seconds	new loss: 5.31789198969
epoch 7, learning rate 0.0455	instance 1000	epoch done in 73.52 seconds	new loss: 5.28786674253
epoch 8, learning rate 0.0417	instance 1000	epoch done in 73.83 seconds	new loss: 5.26503579368
epoch 9, learning rate 0.0385	instance 1000	epoch done in 73.33 seconds	new loss: 5.24442546827
epoch 10, learning rate 0.0357	instance 1000	epoch done in 74.20 seconds	new loss: 5.22779137447

training finished after reaching maximum of 10 epochs
best observed loss was 5.22779137447, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 5
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 8.51580627349

epoch 1, learning rate 0.0500	instance 1000	epoch done in 81.78 seconds	new loss: 6.09443383374
epoch 2, learning rate 0.0417	instance 1000	epoch done in 72.56 seconds	new loss: 5.75726076515
epoch 3, learning rate 0.0357	instance 1000	epoch done in 72.50 seconds	new loss: 5.63928481988
epoch 4, learning rate 0.0312	instance 1000	epoch done in 73.47 seconds	new loss: 5.57463833321
epoch 5, learning rate 0.0278	instance 1000	epoch done in 72.24 seconds	new loss: 5.52215266797
epoch 6, learning rate 0.0250	instance 1000	epoch done in 74.07 seconds	new loss: 5.48523529648
epoch 7, learning rate 0.0227	instance 1000	epoch done in 86.57 seconds	new loss: 5.45225153226
epoch 8, learning rate 0.0208	instance 1000	epoch done in 72.14 seconds	new loss: 5.42692810361
epoch 9, learning rate 0.0192	instance 1000	epoch done in 92.28 seconds	new loss: 5.40562694202
epoch 10, learning rate 0.0179	instance 1000	epoch done in 73.07 seconds	new loss: 5.38765992314

training finished after reaching maximum of 10 epochs
best observed loss was 5.38765992314, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 2
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.35913187834

epoch 1, learning rate 0.5000	instance 1000	epoch done in 58.23 seconds	new loss: 8.27962346115
epoch 2, learning rate 0.4167	instance 1000	epoch done in 57.30 seconds	new loss: 5.7733021895
epoch 3, learning rate 0.3571	instance 1000	epoch done in 57.31 seconds	new loss: 6.1362643158
epoch 4, learning rate 0.3125	instance 1000	epoch done in 57.54 seconds	new loss: 5.50026509454
epoch 5, learning rate 0.2778	instance 1000	epoch done in 57.42 seconds	new loss: 5.32788678809
epoch 6, learning rate 0.2500	instance 1000	epoch done in 56.97 seconds	new loss: 5.25374617575
epoch 7, learning rate 0.2273	instance 1000	epoch done in 57.12 seconds	new loss: 5.19350601543
epoch 8, learning rate 0.2083	instance 1000	epoch done in 57.52 seconds	new loss: 5.16324201396
epoch 9, learning rate 0.1923	instance 1000	epoch done in 57.01 seconds	new loss: 5.14873492612
epoch 10, learning rate 0.1786	instance 1000	epoch done in 57.82 seconds	new loss: 5.11416877258

training finished after reaching maximum of 10 epochs
best observed loss was 5.11416877258, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 2
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 8.48581984964

epoch 1, learning rate 0.1000	instance 1000	epoch done in 57.02 seconds	new loss: 5.81844924726
epoch 2, learning rate 0.0833	instance 1000	epoch done in 57.85 seconds	new loss: 5.63229886177
epoch 3, learning rate 0.0714	instance 1000	epoch done in 56.85 seconds	new loss: 5.53263293936
epoch 4, learning rate 0.0625	instance 1000	epoch done in 57.56 seconds	new loss: 5.43614228519
epoch 5, learning rate 0.0556	instance 1000	epoch done in 57.30 seconds	new loss: 5.38334057155
epoch 6, learning rate 0.0500	instance 1000	epoch done in 57.11 seconds	new loss: 5.34404547823
epoch 7, learning rate 0.0455	instance 1000	epoch done in 57.81 seconds	new loss: 5.30961729737
epoch 8, learning rate 0.0417	instance 1000	epoch done in 57.04 seconds	new loss: 5.28446709499
epoch 9, learning rate 0.0385	instance 1000	epoch done in 57.87 seconds	new loss: 5.26469976591
epoch 10, learning rate 0.0357	instance 1000	epoch done in 57.59 seconds	new loss: 5.24532661362

training finished after reaching maximum of 10 epochs
best observed loss was 5.24532661362, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 2
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 8.60725957241

epoch 1, learning rate 0.0500	instance 1000	epoch done in 57.32 seconds	new loss: 6.18851332182
epoch 2, learning rate 0.0417	instance 1000	epoch done in 56.86 seconds	new loss: 5.80927077441
epoch 3, learning rate 0.0357	instance 1000	epoch done in 57.23 seconds	new loss: 5.66819042289
epoch 4, learning rate 0.0312	instance 1000	epoch done in 58.88 seconds	new loss: 5.58256163093
epoch 5, learning rate 0.0278	instance 1000	epoch done in 60.99 seconds	new loss: 5.5279230602
epoch 6, learning rate 0.0250	instance 1000	epoch done in 58.16 seconds	new loss: 5.4879542438
epoch 7, learning rate 0.0227	instance 1000	epoch done in 58.61 seconds	new loss: 5.45682845008
epoch 8, learning rate 0.0208	instance 1000	epoch done in 62.80 seconds	new loss: 5.43058153151
epoch 9, learning rate 0.0192	instance 1000	epoch done in 58.29 seconds	new loss: 5.40947678732
epoch 10, learning rate 0.0179	instance 1000	epoch done in 57.89 seconds	new loss: 5.39230169741

training finished after reaching maximum of 10 epochs
best observed loss was 5.39230169741, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 0
Initial learning rate set to 0.5, annealing set to 5

calculating initial mean loss on dev set: 8.25420300675

epoch 1, learning rate 0.5000	instance 1000	epoch done in 44.82 seconds	new loss: 10.4449585515
epoch 2, learning rate 0.4167	instance 1000	epoch done in 45.45 seconds	new loss: 8.71121337388
epoch 3, learning rate 0.3571	instance 1000	epoch done in 44.89 seconds	new loss: 6.43914045796
epoch 4, learning rate 0.3125	instance 1000	epoch done in 44.86 seconds	new loss: 7.38183940223
epoch 5, learning rate 0.2778	instance 1000	epoch done in 46.23 seconds	new loss: 6.69513684857
epoch 6, learning rate 0.2500	instance 1000	epoch done in 45.35 seconds	new loss: 6.35630482033
epoch 7, learning rate 0.2273	instance 1000	epoch done in 44.70 seconds	new loss: 5.85593812471
epoch 8, learning rate 0.2083	instance 1000	epoch done in 45.02 seconds	new loss: 5.5026854632
epoch 9, learning rate 0.1923	instance 1000	epoch done in 44.73 seconds	new loss: 5.33966214756
epoch 10, learning rate 0.1786	instance 1000	epoch done in 45.12 seconds	new loss: 5.23435804364

training finished after reaching maximum of 10 epochs
best observed loss was 5.23435804364, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 0
Initial learning rate set to 0.1, annealing set to 5

calculating initial mean loss on dev set: 8.58160863107

epoch 1, learning rate 0.1000	instance 1000	epoch done in 45.42 seconds	new loss: 5.93245121055
epoch 2, learning rate 0.0833	instance 1000	epoch done in 45.17 seconds	new loss: 5.66677831694
epoch 3, learning rate 0.0714	instance 1000	epoch done in 45.01 seconds	new loss: 5.55976425585
epoch 4, learning rate 0.0625	instance 1000	epoch done in 44.95 seconds	new loss: 5.39485538679
epoch 5, learning rate 0.0556	instance 1000	epoch done in 46.10 seconds	new loss: 5.34306577067
epoch 6, learning rate 0.0500	instance 1000	epoch done in 45.22 seconds	new loss: 5.30944141603
epoch 7, learning rate 0.0455	instance 1000	epoch done in 45.54 seconds	new loss: 5.27779356241
epoch 8, learning rate 0.0417	instance 1000	epoch done in 53.49 seconds	new loss: 5.25258543189
epoch 9, learning rate 0.0385	instance 1000	epoch done in 45.26 seconds	new loss: 5.23341629151
epoch 10, learning rate 0.0357	instance 1000	epoch done in 44.88 seconds	new loss: 5.2207631739

training finished after reaching maximum of 10 epochs
best observed loss was 5.2207631739, at epoch 10
setting U, V, W to matrices from best epoch

Training model for 10 epochs
training set: 1000 sentences (batch size 100)
Optimizing loss on 1000 sentences
Vocab size: 2000
Hidden units: 50
Steps for back propagation: 0
Initial learning rate set to 0.05, annealing set to 5

calculating initial mean loss on dev set: 8.41712216528

epoch 1, learning rate 0.0500	instance 1000	epoch done in 44.89 seconds	new loss: 6.13359224785
epoch 2, learning rate 0.0417	instance 1000	epoch done in 45.26 seconds	new loss: 5.83566087036
epoch 3, learning rate 0.0357	instance 1000	epoch done in 44.94 seconds	new loss: 5.71337042781
epoch 4, learning rate 0.0312	instance 1000	epoch done in 44.79 seconds	new loss: 5.63866530163
epoch 5, learning rate 0.0278	instance 1000	epoch done in 44.87 seconds	new loss: 5.59101318144
epoch 6, learning rate 0.0250	instance 1000	epoch done in 45.17 seconds	new loss: 5.5490324445
epoch 7, learning rate 0.0227	instance 1000	epoch done in 45.21 seconds	new loss: 5.51776335714
epoch 8, learning rate 0.0208	instance 1000	epoch done in 45.13 seconds	new loss: 5.49231520941
epoch 9, learning rate 0.0192	instance 1000	epoch done in 45.89 seconds	new loss: 5.46959002448
epoch 10, learning rate 0.0179	instance 1000	epoch done in 45.02 seconds	new loss: 5.45029983086

training finished after reaching maximum of 10 epochs
best observed loss was 5.45029983086, at epoch 10
setting U, V, W to matrices from best epoch