{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from question1 import *\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "[1, 0, 1, 0, 2]\n",
      "[1, 0, 1, 0, 2]\n",
      "[1, 2, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0]\n",
      "[(0, 1), (2, 1), (4, 2)]\n",
      "[(0, 1), (2, 1), (4, 2)]\n",
      "[(0, 1), (1, 2), (4, 1)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "arg1 = 'a'\n",
    "arg2 = 'data/vocabulary.txt'\n",
    "arg3 = 'data/word_contexts.txt'\n",
    "\n",
    "'''\n",
    "(b) function cosine_similarity to calculate similarity between 2 vectors\n",
    "input: vector1\n",
    "input: vector2\n",
    "output: cosine similarity between vector1 and vector2 as a real number\n",
    "'''\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \n",
    "    # Make sure inputs are full.\n",
    "    vector1 = sparse2full(vector1)\n",
    "    vector2 = sparse2full(vector2)\n",
    "    \n",
    "    def dot_prod(v1, v2):\n",
    "        return sum([x * y for x, y in zip(v1, v2)])\n",
    "\n",
    "    num = dot_prod(vector1, vector2)\n",
    "    denom = dot_prod(vector1, vector1)**0.5 * dot_prod(vector2, vector2)**0.5\n",
    "    \n",
    "    return num / float(denom)\n",
    "\n",
    "'''\n",
    "(d) function tf_idf to turn existing frequency-based vector model into tf-idf-based vector model\n",
    "input: freqVectors, a list of frequency-based vectors\n",
    "output: tfIdfVectors, a list of tf-idf-based vectors\n",
    "'''\n",
    "def tf_idf(freqVectors):\n",
    "    \n",
    "    tfIdfVectors = []\n",
    "    N = len(freqVectors)\n",
    "    \n",
    "    # Convert to sparse vectors.\n",
    "    freqVectors = [full2sparse(fv) for fv in freqVectors]\n",
    "    \n",
    "    # Define function that calculates TF-IDF for a single term.\n",
    "    def tf_idf_word(tf, N, df):\n",
    "        return (1 + math.log(tf, 2)) * (1 + math.log(N / float(df), 2))\n",
    "\n",
    "    # Generate dictionary df (document frequency of term). Do this by iterating over documents.\n",
    "    df_dict = {}\n",
    "    for fv in freqVectors:\n",
    "        for term, _ in fv:  # don't need frequency\n",
    "            df_dict[term] = df_dict.get(term, 0) + 1\n",
    "        \n",
    "    for fv in freqVectors:\n",
    "        tfIdfVectors.append([(term, tf_idf_word(tf, N, df_dict[term])) for term, tf in fv])\n",
    "\n",
    "    return tfIdfVectors\n",
    "\n",
    "sparse1 = [(0,1), (2,1), (4,2)]\n",
    "sparse2 = [(0,1), (1,2), (4,1)]\n",
    "sparse3 = []\n",
    "full1 = [1, 0, 1, 0, 2]\n",
    "full2 = [1, 2, 0, 0, 1]\n",
    "full3 = [0, 0, 0, 0, 0]\n",
    "\n",
    "def is_sparse(vector):\n",
    "    return not vector or isinstance(vector[0], tuple)\n",
    "\n",
    "def sparse2full(sparse, length=5000):\n",
    "    sparse = list(sparse)  # in case it's a numpy array?\n",
    "    if not is_sparse(sparse):\n",
    "        return sparse\n",
    "    if length is None:\n",
    "        length = max([tup[0] for tup in sparse]) + 1\n",
    "    full = [0] * length\n",
    "    for tup in sparse:\n",
    "        full[tup[0]] = tup[1]\n",
    "    return full\n",
    "\n",
    "def full2sparse(full):\n",
    "    if is_sparse(full):\n",
    "        return full\n",
    "    sparse = [(c, n) for c, n in enumerate(full) if n > 0]\n",
    "    return sparse\n",
    "\n",
    "print is_sparse(sparse1)\n",
    "print is_sparse(sparse3)\n",
    "print is_sparse(full1)\n",
    "\n",
    "print sparse2full(sparse1, 5)\n",
    "print sparse2full(full1, 5)\n",
    "print sparse2full(sparse2, 5)\n",
    "print sparse2full(sparse3, 5)\n",
    "\n",
    "print full2sparse(full1)\n",
    "print full2sparse(sparse1)\n",
    "print full2sparse(full2)\n",
    "print full2sparse(full3)\n",
    "\n",
    "'''\n",
    "(a) function load_corpus to read a corpus from disk\n",
    "input: vocabFile containing vocabulary\n",
    "input: contextFile containing word contexts\n",
    "output: id2word mapping word IDs to words\n",
    "output: word2id mapping words to word IDs\n",
    "output: vectors for the corpus, as a list of sparse vectors\n",
    "'''\n",
    "def load_corpus(vocabFile, contextFile):\n",
    "    id2word = {}\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "\n",
    "    with open(vocabFile, 'r') as f:\n",
    "        vocab = f.read().splitlines()\n",
    "    \n",
    "    with open(contextFile, 'r') as f:\n",
    "        context = f.read().splitlines()\n",
    "    \n",
    "    for big_string in context:\n",
    "        tups = []\n",
    "        if big_string[0] != '0':  # words without context words have a string that starts with '0'\n",
    "            cns = big_string.split(' ')[1:]\n",
    "            for cn in cns:\n",
    "                c, n = cn.split(':')\n",
    "                tups.append((int(c), int(n)))\n",
    "        vectors.append(tups)\n",
    "    \n",
    "    for ID, word in enumerate(vocab):\n",
    "        id2word[ID] = word\n",
    "        word2id[word] = ID\n",
    "\n",
    "    return id2word, word2id, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1\n",
      "/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1\n"
     ]
    }
   ],
   "source": [
    "# Fix paths\n",
    "import os\n",
    "print os.getcwd()  # '/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1'\n",
    "os.chdir('/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1')\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3], [4, 5, 6])\n",
      "([1, 2, 3, 0], [4, 5, 6, 7])\n",
      "([1, 2, 3, 10], [4, 5, 6, 0])\n"
     ]
    }
   ],
   "source": [
    "def equalize_full_lens(v1, v2):  \n",
    "    \n",
    "    diff = len(v2) - len(v1)\n",
    "    if diff > 0:\n",
    "        v1.extend([0] * diff)\n",
    "    else:\n",
    "        v2.extend([0] * -diff)\n",
    "        \n",
    "    return v1, v2\n",
    "\n",
    "print equalize_full_lens([1, 2, 3], [4, 5, 6])\n",
    "print equalize_full_lens([1, 2, 3], [4, 5, 6, 7])\n",
    "print equalize_full_lens([1, 2, 3, 10], [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('data/test.txt', 'r') as f:\n",
    "    sents_raw = f.read().splitlines()\n",
    "sents = [json.loads(s) for s in sents_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents:\n",
      "\t{u'target_position': u'29', u'target_word': u'side.n', u'id': u'301', u'sentence': u'on.i Sunday.n at.i Craven.n Cottage.n ,.x Jose.n Mourinho.n and.c his.d all.r star.v exhibit.v all.r of.i the.x above.a symptom.n and.c they.d be.v make.v to.x pay.v the.x price.n by.i a.x Fulham.n side.n that.x have.v in.i previous.a week.n wake.v up.x after.i match.n with.i their.d head.n kick.v in.i ..x'}\n",
      "=========================\n",
      "target_position:\n",
      "\t29\n",
      "id:\n",
      "\t301\n",
      "=========================\n",
      "sentence:\n",
      "\ton.i Sunday.n at.i Craven.n Cottage.n ,.x Jose.n Mourinho.n and.c his.d all.r star.v exhibit.v all.r of.i the.x above.a symptom.n and.c they.d be.v make.v to.x pay.v the.x price.n by.i a.x Fulham.n side.n that.x have.v in.i previous.a week.n wake.v up.x after.i match.n with.i their.d head.n kick.v in.i ..x\n",
      "=========================\n",
      "on.i\n",
      "sunday.n\n",
      "at.i\n",
      "craven.n\n",
      "cottage.n\n",
      ",.x\n",
      "jose.n\n",
      "mourinho.n\n",
      "and.c\n",
      "his.d\n",
      "all.r\n",
      "star.v\n",
      "exhibit.v\n",
      "all.r\n",
      "of.i\n",
      "the.x\n",
      "above.a\n",
      "symptom.n\n",
      "and.c\n",
      "they.d\n",
      "be.v\n",
      "make.v\n",
      "to.x\n",
      "pay.v\n",
      "the.x\n",
      "price.n\n",
      "by.i\n",
      "a.x\n",
      "fulham.n\n",
      "side.n\n",
      "that.x\n",
      "have.v\n",
      "in.i\n",
      "previous.a\n",
      "week.n\n",
      "wake.v\n",
      "up.x\n",
      "after.i\n",
      "match.n\n",
      "with.i\n",
      "their.d\n",
      "head.n\n",
      "kick.v\n",
      "in.i\n",
      "..x\n"
     ]
    }
   ],
   "source": [
    "print 'sents:\\n\\t{}'.format(sents[0])\n",
    "print '=' * 25\n",
    "print 'target_position:\\n\\t{}'.format(sents[0]['target_position'])\n",
    "print 'id:\\n\\t{}'.format(sents[0]['id'])\n",
    "print '=' * 25\n",
    "print 'sentence:\\n\\t{}'.format(sents[0]['sentence'])\n",
    "print '=' * 25\n",
    "for w in sents[0]['sentence'].split(' '):\n",
    "    print w.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "helper class to load a thesaurus from disk\n",
    "input: thesaurusFile, file on disk containing a thesaurus of substitution words for targets\n",
    "output: the thesaurus, as a mapping from target words to lists of substitution words\n",
    "'''\n",
    "def load_thesaurus(thesaurusFile):\n",
    "    thesaurus = {}\n",
    "    with open(thesaurusFile) as inFile:\n",
    "        for line in inFile.readlines():\n",
    "            word, subs = line.strip().split(\"\\t\")\n",
    "            thesaurus[word] = subs.split(\" \")\n",
    "    return thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(a) function addition for adding 2 vectors\n",
    "input: vector1\n",
    "input: vector2\n",
    "output: addVector, the resulting vector when adding vector1 and vector2\n",
    "'''\n",
    "def addition(vector1, vector2):\n",
    "    \n",
    "    # Make sure inputs are full.\n",
    "    vector1 = sparse2full(vector1, None)\n",
    "    vector2 = sparse2full(vector2, None)\n",
    "    vector1, vector2 = equalize_full_lens(vector1, vector2)\n",
    "    \n",
    "    added = [x + y for x, y in zip(vector1, vector2)]\n",
    "    \n",
    "    return full2sparse(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(a) function multiplication for multiplying 2 vectors\n",
    "input: vector1\n",
    "input: vector2\n",
    "output: mulVector, the resulting vector when multiplying vector1 and vector2\n",
    "'''\n",
    "def multiplication(vector1, vector2):\n",
    "    \n",
    "    # Make sure inputs are full.\n",
    "    vector1 = sparse2full(vector1, None)\n",
    "    vector2 = sparse2full(vector2, None)\n",
    "    vector1, vector2 = equalize_full_lens(vector1, vector2)\n",
    "    \n",
    "    multiplied = [x * y for x, y in zip(vector1, vector2)]\n",
    "    \n",
    "    return full2sparse(multiplied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a): vector addition and multiplication\n",
      "\tPass: sparse addition\n",
      "\tPass: sparse multiplication\n",
      "\tPass: full addition\n",
      "\tPass: full multiplication\n"
     ]
    }
   ],
   "source": [
    "print(\"(a): vector addition and multiplication\")\n",
    "v1, v2, v3 , v4 = [(0,1), (2,1), (4,2)], [(0,1), (1,2), (4,1)], [1, 0, 1, 0, 2], [1, 2, 0, 0, 1]\n",
    "try:\n",
    "    if not set(addition(v1, v2)) == set([(0, 2), (2, 1), (4, 3), (1, 2)]):\n",
    "        print(\"\\tError: sparse addition returned wrong result\")\n",
    "    else:\n",
    "        print(\"\\tPass: sparse addition\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: exception raised in sparse addition\")\n",
    "    print(e)\n",
    "try:\n",
    "    if not set(multiplication(v1, v2)) == set([(0,1), (4,2)]):\n",
    "        print(\"\\tError: sparse multiplication returned wrong result\")\n",
    "    else:\n",
    "        print(\"\\tPass: sparse multiplication\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: exception raised in sparse multiplication\")\n",
    "    print(e)\n",
    "try:\n",
    "    addition(v3,v4)\n",
    "    print(\"\\tPass: full addition\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: exception raised in full addition\")\n",
    "    print(e)\n",
    "try:\n",
    "    multiplication(v3,v4)\n",
    "    print(\"\\tPass: full multiplication\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: exception raised in full addition\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 0.022655528236718809), (29, 0.013911389361581955), (39, 0.059216902975083245), (43, 0.027167515547374756), (53, 0.013895648531032241), (59, 0.042801687098991341), (63, 0.52349675012988228), (75, 0.026278672983501009), (84, 0.016912568969295674), (87, 0.079649205504056811), (93, 0.013221198967828706), (97, 0.023422562524206623)]\n",
      "=========================\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-44dd95357c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'='\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1008\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model_lda = gensim.models.ldamodel.LdaModel.load('run/lda_model')\n",
    "# model_lda.get_term_topics(0)\n",
    "w_vect = frequencyVectors[word2id[\"house.n\"]]\n",
    "tmp = model_lda.get_document_topics(w_vect)\n",
    "print tmp\n",
    "print '=' * 25\n",
    "print [tup[1] for tup in tmp if tup[0] == 1008][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(d) function prob_z_given_w to get probability of LDA topic z, given target word w\n",
    "input: ldaModel\n",
    "input: topicID as an integer\n",
    "input: wordVector in frequency space\n",
    "output: probability of the topic with topicID in the ldaModel, given the wordVector\n",
    "'''\n",
    "def prob_z_given_w(ldaModel, topicID, wordVector):\n",
    "    topic_probs = ldaModel.get_document_topics(wordVector, minimum_probability=0.)\n",
    "    try:\n",
    "        prob_topic = [tup[1] for tup in topic_probs if tup[0]==topicID][0]\n",
    "    except IndexError:\n",
    "        prob_topic = 0.\n",
    "    return prob_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(d) function prob_w_given_z to get probability of target word w, given LDA topic z\n",
    "input: ldaModel\n",
    "input: targetWord as a string\n",
    "input: topicID as an integer\n",
    "output: probability of the targetWord, given the topic with topicID in the ldaModel\n",
    "'''\n",
    "def prob_w_given_z(ldaModel, targetWord, topicID):\n",
    "    words = ldaModel.show_topic(topicID, 20000)  # 20000 gives all\n",
    "    try:\n",
    "        word_prob = [tup[1] for tup in words if gensim.utils.any2unicode(tup[0])==targetWord][0]\n",
    "    except IndexError:\n",
    "        word_prob = 0.\n",
    "    return word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0228965307727\n",
      "0.00195658399623\n"
     ]
    }
   ],
   "source": [
    "print prob_z_given_w(ldaModel, houseTopic, vectors[word2id[\"house.n\"]])\n",
    "print prob_w_given_z(ldaModel, \"house.n\", houseTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d): calculating P(Z|w) and P(w|Z)\n",
      "\tloading corpus\n",
      "\tloading LDA model\n",
      "\tPass: P(Z|w)\n",
      "\tPass: P(w|Z)\n"
     ]
    }
   ],
   "source": [
    "print(\"(d): calculating P(Z|w) and P(w|Z)\")\n",
    "print(\"\\tloading corpus\")\n",
    "id2word,word2id,vectors=load_corpus(arg2, arg3)\n",
    "print(\"\\tloading LDA model\")\n",
    "ldaModel = gensim.models.ldamodel.LdaModel.load(\"run/lda_model\")\n",
    "houseTopic = ldaModel[vectors[word2id[\"house.n\"]]][0][0]\n",
    "try:\n",
    "    if prob_z_given_w(ldaModel, houseTopic, vectors[word2id[\"house.n\"]]) > 0.0:\n",
    "        print(\"\\tPass: P(Z|w)\")\n",
    "    else:\n",
    "        print(\"\\tFail: P(Z|w)\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: exception during P(Z|w)\")\n",
    "    print(e)\n",
    "try:\n",
    "    if prob_w_given_z(ldaModel, \"house.n\", houseTopic) > 0.0:\n",
    "        print(\"\\tPass: P(w|Z)\")\n",
    "    else:\n",
    "        print(\"\\tFail: P(w|Z)\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: exception during P(w|Z)\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['somewhat.r', 'round.r', 'approximately.r', 'roughly.r', 'around.r', 'of.r', 'nearly.r']\n",
      "['access.n', 'explanation.n', 'subscription.n', 'balance.n', 'facility.n', 'description.n', 'asset.n', 'consideration.n', 'narrative.n', 'report.n', 'fund.n', 'statement.n', 'finance.n']\n",
      "[u'purchase.v', u'gather.v', u'gain.v', u'secure.v', u'achieve.v', u'obtain.v', u'get.v', u'receive.v', u'learn.v', u'buy.v', u'collect.v', u'find.v']\n"
     ]
    }
   ],
   "source": [
    "print load_thesaurus('data/test_thesaurus.txt')['about.r']\n",
    "print load_thesaurus('data/test_thesaurus.txt')['account.n']\n",
    "print [gensim.utils.any2unicode(w) for w in load_thesaurus('data/test_thesaurus.txt')['acquire.v']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# (f) get the best substitution word in a given sentence, according to a given model (tf-idf, word2vec, LDA) and type (addition, multiplication, lda)\n",
    "# input: jsonSentence, a string in json format\n",
    "# input: thesaurus, mapping from target words to candidate substitution words\n",
    "# input: word2id, mapping from vocabulary words to word IDs\n",
    "# input: model, a vector space, Word2Vec or LDA model\n",
    "# input: frequency vectors, original frequency vectors (for querying LDA model)\n",
    "# input: csType, a string indicating the method of calculating context sensitive vectors: \"addition\", \"multiplication\", or \"lda\"\n",
    "# output: the best substitution word for the jsonSentence in the given model, using the given csType\n",
    "# '''\n",
    "# def best_substitute(jsonSentence, thesaurus, word2id, model, frequencyVectors, csType, is_debug=False, vocab_unicode=None):\n",
    "    \n",
    "#     window = 5\n",
    "    \n",
    "#     target_word = jsonSentence['target_word'].lower()\n",
    "#     target_position = int(jsonSentence['target_position'])\n",
    "#     sentence = [w.lower() for w in jsonSentence['sentence'].split(' ')]\n",
    "#     words = thesaurus[target_word]\n",
    "    \n",
    "#     prob_z_given_w_dict = {}\n",
    "#     prob_w_given_z_dict = {}\n",
    "    \n",
    "#     # (b) use addition to get context sensitive vectors\n",
    "#     if csType == \"addition\":\n",
    "#         def context_sensitive(v1, v2, context=None, target_word=None):\n",
    "#             return addition(v1, v2)\n",
    "\n",
    "#     # (c) use multiplication to get context sensitive vectors\n",
    "#     elif csType == \"multiplication\":\n",
    "#         def context_sensitive(v1, v2, context=None, target_word=None):\n",
    "#             return multiplication(v1, v2)\n",
    "    \n",
    "#     # (d) use LDA to get context sensitive vectors\n",
    "#     elif csType == \"lda\":\n",
    "#         topicIds = [lst[0] for lst in model.show_topics(-1)]  # -1 gives all topics\n",
    "#         def context_sensitive(t, c, context, target_word):\n",
    "#             cs_vector = []\n",
    "#             for topicId in topicIds:\n",
    "#                 try:\n",
    "#                     prob_z_given_w_value = prob_z_given_w_dict[(topicId, target_word)]\n",
    "#                     # print '        skipping prob_z_given_w calculation...'\n",
    "#                 except KeyError:\n",
    "#                     # print '        calculating prob_z_given_w value...'\n",
    "#                     # t0 = time.time()\n",
    "#                     prob_z_given_w_value = prob_z_given_w(model, topicId, frequencyVectors[word2id[target_word]])\n",
    "#                     prob_z_given_w_dict[(topicId, target_word)] = prob_z_given_w_value\n",
    "#                     # print '            time: {}'.format(time.time() - t0)\n",
    "#                 try:\n",
    "#                     prob_w_given_z_value = prob_w_given_z_dict[(topicId, context)]\n",
    "#                     # print '        skipping prob_w_given_z calculation...'\n",
    "#                 except KeyError:\n",
    "#                     # t0 = time.time()\n",
    "#                     # print '        calculating prob_w_given_z value...'\n",
    "#                     prob_w_given_z_value = prob_w_given_z(model, context, topicId)\n",
    "#                     prob_w_given_z_dict[(topicId, context)] = prob_w_given_z_value\n",
    "#                     # print '            time: {}'.format(time.time() - t0)\n",
    "#                 cs_vector.append(prob_z_given_w_value * prob_w_given_z_value)\n",
    "#             return cs_vector\n",
    "    \n",
    "#     if vocab_unicode is None:\n",
    "#         vocab_unicode = [gensim.utils.any2unicode(w) for w in word2id.keys()]\n",
    "    \n",
    "#     contexts = []\n",
    "#     for i in range(target_position - window, target_position + window + 1):\n",
    "#         if i != target_position and i >= 0 and i < len(sentence) and sentence[i] in vocab_unicode:\n",
    "#             contexts.append(sentence[i])\n",
    "#     if not contexts:\n",
    "#         return None  # fail to predict if no context words\n",
    "    \n",
    "#     def get_vector(model, target_word):\n",
    "#         if csType == 'lda':\n",
    "#             vector = frequencyVectors[word2id[target_word]]  # no need for model\n",
    "#         else:\n",
    "#             if type(model) == gensim.models.word2vec.Word2Vec:\n",
    "#                 vector = list(model[target_word])\n",
    "#             else:\n",
    "#                 vector = model[word2id[target_word]]\n",
    "#         return vector\n",
    "    \n",
    "#     best_sub = None\n",
    "#     best_score = 0.\n",
    "#     t = get_vector(model, target_word)\n",
    "#     for word in words:\n",
    "#         # print 'word: {}'.format(word)\n",
    "#         w = get_vector(model, word)\n",
    "#         score = 0.\n",
    "#         for context in contexts:\n",
    "#             # print 'context: {}'.format(context)\n",
    "#             # print '    getting vector...'\n",
    "#             try:\n",
    "#                 c = get_vector(model, context)\n",
    "#             except KeyError:  # e.g., u'continually.r': word not in vocab\n",
    "#                 continue\n",
    "#             if not c:  # context word has no vector\n",
    "#                 continue\n",
    "#             # print '    calculating context sensitivity...'\n",
    "#             tc = context_sensitive(t, c, context, target_word)\n",
    "#             if not tc:  # sometimes multiplication returns []; then ignore context word\n",
    "#                 continue\n",
    "#             score += cosine_similarity(w, tc)\n",
    "#         if score > best_score:\n",
    "#             best_score = score\n",
    "#             best_sub = word\n",
    "    \n",
    "#     return best_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_substitute(jsonSentence, thesaurus, word2id, model, frequencyVectors, csType, vocab_unicode=None):\n",
    "    \n",
    "    window = 5\n",
    "    \n",
    "    target_word = jsonSentence['target_word'].lower()\n",
    "    target_position = int(jsonSentence['target_position'])\n",
    "    sentence = [w.lower() for w in jsonSentence['sentence'].split(' ')]\n",
    "    words = thesaurus[target_word]\n",
    "    \n",
    "    if vocab_unicode is None:\n",
    "        vocab_unicode = [gensim.utils.any2unicode(w) for w in word2id.keys()]\n",
    "\n",
    "    # Dicts are necessary for a reasonable run time given how this had been coded.\n",
    "    # Otherwise the prob_z_given_w and prob_w_given_z calculations each take\n",
    "    # 0.02-0.15 seconds, and they must be performed *many* times (200 sentences\n",
    "    # * ~5 thesaurus words * <=10 context words * 100 topics = ~1 million).\n",
    "    prob_z_given_w_dict = {}\n",
    "    prob_w_given_z_dict = {}\n",
    "    \n",
    "    # (b) use addition to get context sensitive vectors\n",
    "    if csType == \"addition\":\n",
    "        def context_sensitive(v1, v2, context=None, target_word=None):\n",
    "            return addition(v1, v2)\n",
    "\n",
    "    # (c) use multiplication to get context sensitive vectors\n",
    "    elif csType == \"multiplication\":\n",
    "        def context_sensitive(v1, v2, context=None, target_word=None):\n",
    "            return multiplication(v1, v2)\n",
    "    \n",
    "    # (d) use LDA to get context sensitive vectors\n",
    "    elif csType == \"lda\":\n",
    "        topicIds = [lst[0] for lst in model.show_topics(-1)]  # -1 gives all topics\n",
    "        def context_sensitive(t, c, context, target_word):\n",
    "            cs_vector = []\n",
    "            for topicId in topicIds:\n",
    "                # Get prob_z_given_w.\n",
    "                try:\n",
    "                    prob_z_given_w_value = prob_z_given_w_dict[(topicId, target_word)]\n",
    "                except KeyError:\n",
    "                    prob_z_given_w_value = prob_z_given_w(model, topicId, frequencyVectors[word2id[target_word]])\n",
    "                    prob_z_given_w_dict[(topicId, target_word)] = prob_z_given_w_value\n",
    "                # Get prob_w_given_z.\n",
    "                try:\n",
    "                    prob_w_given_z_value = prob_w_given_z_dict[(topicId, context)]\n",
    "                except KeyError:\n",
    "                    prob_w_given_z_value = prob_w_given_z(model, context, topicId)\n",
    "                    prob_w_given_z_dict[(topicId, context)] = prob_w_given_z_value\n",
    "                # Add their product to vector.\n",
    "                cs_vector.append(prob_z_given_w_value * prob_w_given_z_value)\n",
    "            return cs_vector\n",
    "    \n",
    "    contexts = []\n",
    "    for i in range(target_position - window, target_position + window + 1):\n",
    "        if i != target_position and i >= 0 and i < len(sentence) and sentence[i] in vocab_unicode:\n",
    "            contexts.append(sentence[i])\n",
    "    if not contexts:\n",
    "        return None  # fail to predict if no context words\n",
    "    \n",
    "    def get_vector(model, target_word):\n",
    "        if csType == 'lda':\n",
    "            vector = frequencyVectors[word2id[target_word]]  # no need for model\n",
    "        else:\n",
    "            if type(model) == gensim.models.word2vec.Word2Vec:\n",
    "                vector = list(model[target_word])\n",
    "            else:\n",
    "                vector = model[word2id[target_word]]\n",
    "        return vector\n",
    "    \n",
    "    best_word = None\n",
    "    best_score = 0.\n",
    "    t = get_vector(model, target_word)\n",
    "    for word in words:\n",
    "        w = get_vector(model, word)\n",
    "        score = 0.\n",
    "        for context in contexts:\n",
    "            try:\n",
    "                c = get_vector(model, context)\n",
    "            except KeyError:  # e.g., u'continually.r': word not in vocab\n",
    "                continue\n",
    "            if not c:  # context word has no vector\n",
    "                continue\n",
    "            tc = context_sensitive(t, c, context, target_word)\n",
    "            if not tc:  # sometimes multiplication returns []; then ignore context word\n",
    "                continue\n",
    "            score += cosine_similarity(w, tc)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_word = word\n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word, word2id, frequencyVectors = load_corpus('data/vocabulary.txt', 'data/word_contexts.txt')\n",
    "# jsonSentence = sents[3]\n",
    "thesaurus = load_thesaurus('data/test_thesaurus.txt')\n",
    "# csType = 'addition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tf_idf(frequencyVectors)\n",
    "print best_substitute(jsonSentence, thesaurus, word2id, model, frequencyVectors, csType, is_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.word2vec.Word2Vec.load('run/word2vec_model')\n",
    "print best_substitute(jsonSentence, thesaurus, word2id, model, frequencyVectors, csType, is_debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "run/output_lda.txt\n",
      "=========================\n",
      "side.n 301 :: position\n",
      "side.n 302 :: position\n",
      "side.n 303 :: position\n",
      "side.n 304 :: position\n",
      "side.n 305 :: position\n",
      "side.n 306 :: position\n",
      "side.n 307 :: \n",
      "side.n 308 :: position\n",
      "side.n 309 :: position\n",
      "side.n 310 :: shore\n",
      "tell.v 311 :: order\n",
      "tell.v 312 :: explain\n",
      "tell.v 313 :: explain\n",
      "tell.v 314 :: explain\n",
      "tell.v 315 :: explain\n",
      "tell.v 316 :: explain\n",
      "tell.v 317 :: explain\n",
      "tell.v 318 :: notify\n",
      "tell.v 319 :: explain\n",
      "tell.v 320 :: explain\n",
      "terrible.a 321 :: negative\n",
      "terrible.a 322 :: negative\n",
      "terrible.a 323 :: negative\n",
      "terrible.a 324 :: negative\n",
      "terrible.a 325 :: bad\n",
      "terrible.a 326 :: bad\n",
      "terrible.a 327 :: negative\n",
      "terrible.a 328 :: negative\n",
      "terrible.a 329 :: bad\n",
      "terrible.a 330 :: negative\n",
      "think.v 331 :: contemplate\n",
      "think.v 332 :: contemplate\n",
      "think.v 333 :: contemplate\n",
      "think.v 334 :: contemplate\n",
      "think.v 335 :: contemplate\n",
      "think.v 336 :: contemplate\n",
      "think.v 337 :: contemplate\n",
      "think.v 338 :: contemplate\n",
      "think.v 339 :: contemplate\n",
      "think.v 340 :: contemplate\n",
      "thus.r 341 :: so\n",
      "thus.r 342 :: so\n",
      "thus.r 343 :: so\n",
      "thus.r 344 :: so\n",
      "thus.r 345 :: so\n",
      "thus.r 346 :: so\n",
      "thus.r 347 :: so\n",
      "thus.r 348 :: so\n",
      "thus.r 349 :: so\n",
      "thus.r 350 :: so\n",
      "wind.n 351 :: air\n",
      "wind.n 352 :: air\n",
      "wind.n 353 :: air\n",
      "wind.n 354 :: air\n",
      "wind.n 355 :: air\n",
      "wind.n 356 :: air\n",
      "wind.n 357 :: air\n",
      "wind.n 358 :: air\n",
      "wind.n 359 :: air\n",
      "wind.n 360 :: air\n",
      "charge.v 361 :: demand\n",
      "charge.v 362 :: require\n",
      "charge.v 363 :: require\n",
      "charge.v 364 :: cost\n",
      "charge.v 365 :: cost\n",
      "charge.v 366 :: require\n",
      "charge.v 367 :: require\n",
      "charge.v 368 :: require\n",
      "charge.v 369 :: demand\n",
      "charge.v 370 :: require\n",
      "civil.a 371 :: individual\n",
      "civil.a 372 :: individual\n",
      "civil.a 373 :: individual\n",
      "civil.a 374 :: individual\n",
      "civil.a 375 :: individual\n",
      "civil.a 376 :: individual\n",
      "civil.a 377 :: individual\n",
      "civil.a 378 :: individual\n",
      "civil.a 379 :: individual\n",
      "civil.a 380 :: individual\n",
      "clean.v 381 :: remove\n",
      "clean.v 382 :: remove\n",
      "clean.v 383 :: remove\n",
      "clean.v 384 :: wash\n",
      "clean.v 385 :: remove\n",
      "clean.v 386 :: remove\n",
      "clean.v 387 :: remove\n",
      "clean.v 388 :: remove\n",
      "clean.v 389 :: remove\n",
      "clean.v 390 :: wash\n",
      "coach.n 391 :: car\n",
      "coach.n 392 :: car\n",
      "coach.n 393 :: car\n",
      "coach.n 394 :: bus\n",
      "coach.n 395 :: bus\n",
      "coach.n 396 :: car\n",
      "coach.n 397 :: car\n",
      "coach.n 398 :: car\n",
      "coach.n 399 :: car\n",
      "coach.n 400 :: car\n",
      "dry.a 401 :: empty\n",
      "dry.a 402 :: empty\n",
      "dry.a 403 :: empty\n",
      "dry.a 404 :: empty\n",
      "dry.a 405 :: empty\n",
      "dry.a 406 :: empty\n",
      "dry.a 407 :: empty\n",
      "dry.a 408 :: empty\n",
      "dry.a 409 :: empty\n",
      "dry.a 410 :: empty\n",
      "full.a 411 :: whole\n",
      "full.a 412 :: whole\n",
      "full.a 413 :: whole\n",
      "full.a 414 :: whole\n",
      "full.a 415 :: whole\n",
      "full.a 416 :: entire\n",
      "full.a 417 :: whole\n",
      "full.a 418 :: whole\n",
      "full.a 419 :: whole\n",
      "full.a 420 :: big\n",
      "heap.n 421 :: pile\n",
      "heap.n 422 :: pile\n",
      "heap.n 423 :: pile\n",
      "heap.n 424 :: pile\n",
      "heap.n 425 :: pile\n",
      "heap.n 426 :: pile\n",
      "heap.n 427 :: pile\n",
      "heap.n 428 :: pile\n",
      "heap.n 429 :: pile\n",
      "heap.n 430 :: pile\n",
      "job.n 431 :: bit\n",
      "job.n 432 :: bit\n",
      "job.n 433 :: bit\n",
      "job.n 434 :: bit\n",
      "job.n 435 :: bit\n",
      "job.n 436 :: bit\n",
      "job.n 437 :: bit\n",
      "job.n 438 :: bit\n",
      "job.n 439 :: bit\n",
      "job.n 440 :: bit\n",
      "late.r 441 :: recently\n",
      "late.r 442 :: behind\n",
      "late.r 443 :: recently\n",
      "late.r 444 :: behind\n",
      "late.r 445 :: behind\n",
      "late.r 446 :: recently\n",
      "late.r 447 :: behind\n",
      "late.r 448 :: recently\n",
      "late.r 449 :: behind\n",
      "late.r 450 :: behind\n",
      "new.a 451 :: other\n",
      "new.a 452 :: novel\n",
      "new.a 453 :: different\n",
      "new.a 454 :: other\n",
      "new.a 455 :: different\n",
      "new.a 456 :: other\n",
      "new.a 457 :: novel\n",
      "new.a 458 :: novel\n",
      "new.a 459 :: novel\n",
      "new.a 460 :: different\n",
      "often.r 461 :: usually\n",
      "often.r 462 :: usually\n",
      "often.r 463 :: usually\n",
      "often.r 464 :: sometimes\n",
      "often.r 465 :: usually\n",
      "often.r 466 :: usually\n",
      "often.r 467 :: usually\n",
      "often.r 468 :: usually\n",
      "often.r 469 :: usually\n",
      "often.r 470 :: usually\n",
      "only.r 471 :: merely\n",
      "only.r 472 :: merely\n",
      "only.r 473 :: merely\n",
      "only.r 474 :: merely\n",
      "only.r 475 :: merely\n",
      "only.r 476 :: merely\n",
      "only.r 477 :: merely\n",
      "only.r 478 :: merely\n",
      "only.r 479 :: merely\n",
      "only.r 480 :: merely\n",
      "pulse.n 481 :: heart\n",
      "pulse.n 482 :: heart\n",
      "pulse.n 483 :: heart\n",
      "pulse.n 484 :: heart\n",
      "pulse.n 485 :: heart\n",
      "pulse.n 486 :: heart\n",
      "pulse.n 487 :: heart\n",
      "pulse.n 488 :: heart\n",
      "pulse.n 489 :: heart\n",
      "pulse.n 490 :: centre\n",
      "put.v 491 :: place\n",
      "put.v 492 :: place\n",
      "put.v 493 :: stick\n",
      "put.v 494 :: place\n",
      "put.v 495 :: stick\n",
      "put.v 496 :: leave\n",
      "put.v 497 :: allow\n",
      "put.v 498 :: place\n",
      "put.v 499 :: allow\n",
      "put.v 500 :: allow\n"
     ]
    }
   ],
   "source": [
    "vocab_unicode = [gensim.utils.any2unicode(w) for w in word2id.keys()]\n",
    "def write_file(model, csType, filename, vocab_unicode):\n",
    "    print '=' * 25\n",
    "    print filename\n",
    "    print '=' * 25\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        for jsonSentence in sents:\n",
    "            target_word = jsonSentence['target_word']\n",
    "            ID = jsonSentence['id']\n",
    "            best_sub = best_substitute(jsonSentence, thesaurus, word2id, model, frequencyVectors, csType, vocab_unicode=vocab_unicode)\n",
    "            if best_sub is None:\n",
    "                best_sub = ''\n",
    "            output = '{} {} :: {}'.format(target_word, ID, re.sub('\\..*', '', best_sub))\n",
    "            print output\n",
    "            f.write(output + '\\n')\n",
    "\n",
    "# Write file for LDA.\n",
    "write_file(model_lda, 'lda', 'run/{}.txt'.format('output_lda'), vocab_unicode)\n",
    "\n",
    "# Write files for everything else.\n",
    "models = [tf_idf(frequencyVectors), gensim.models.word2vec.Word2Vec.load('run/word2vec_model')]\n",
    "model_strs = ['tf-idf', 'word2vec']\n",
    "csTypes = ['addition', 'multiplication']\n",
    "for model, model_str in zip(models, model_strs):\n",
    "    for csType in csTypes:\n",
    "        filename = 'run/{}_{}.txt'.format(model_str, csType)\n",
    "        write_file(model, csType, filename, vocab_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "testtest.txt\n",
      "=========================\n",
      "side.n 301 :: \n",
      "side.n 302 :: perspective\n",
      "side.n 303 :: position\n",
      "side.n 304 :: view\n",
      "side.n 305 :: responsibility\n",
      "side.n 306 :: part\n",
      "side.n 307 :: \n",
      "side.n 308 :: view\n",
      "side.n 309 :: view\n",
      "side.n 310 :: part\n",
      "tell.v 311 :: assure\n",
      "tell.v 312 :: assure\n",
      "tell.v 313 :: assure\n",
      "tell.v 314 :: assure\n",
      "tell.v 315 :: assure\n",
      "tell.v 316 :: assure\n",
      "tell.v 317 :: assure\n",
      "tell.v 318 :: assure\n",
      "tell.v 319 :: assure\n",
      "tell.v 320 :: assure\n",
      "terrible.a 321 :: severe\n",
      "terrible.a 322 :: negative\n",
      "terrible.a 323 :: negative\n",
      "terrible.a 324 :: negative\n",
      "terrible.a 325 :: severe\n",
      "terrible.a 326 :: dreadful\n",
      "terrible.a 327 :: dreadful\n",
      "terrible.a 328 :: negative\n",
      "terrible.a 329 :: frightening\n",
      "terrible.a 330 :: negative\n",
      "think.v 331 :: feel\n",
      "think.v 332 :: feel\n",
      "think.v 333 :: guess\n",
      "think.v 334 :: guess\n",
      "think.v 335 :: guess\n",
      "think.v 336 :: expect\n",
      "think.v 337 :: suppose\n",
      "think.v 338 :: guess\n",
      "think.v 339 :: feel\n",
      "think.v 340 :: guess\n",
      "thus.r 341 :: hence\n",
      "thus.r 342 :: hence\n",
      "thus.r 343 :: hence\n",
      "thus.r 344 :: hence\n",
      "thus.r 345 :: consequently\n",
      "thus.r 346 :: hence\n",
      "thus.r 347 :: hence\n",
      "thus.r 348 :: hence\n",
      "thus.r 349 :: this\n",
      "thus.r 350 :: there\n",
      "wind.n 351 :: \n",
      "wind.n 352 :: \n",
      "wind.n 353 :: breeze\n",
      "wind.n 354 :: \n",
      "wind.n 355 :: breeze\n",
      "wind.n 356 :: gas\n",
      "wind.n 357 :: gas\n",
      "wind.n 358 :: \n",
      "wind.n 359 :: breeze\n",
      "wind.n 360 :: \n",
      "charge.v 361 :: dash\n",
      "charge.v 362 :: demand\n",
      "charge.v 363 :: ask\n",
      "charge.v 364 :: storm\n",
      "charge.v 365 :: demand\n",
      "charge.v 366 :: ask\n",
      "charge.v 367 :: dash\n",
      "charge.v 368 :: demand\n",
      "charge.v 369 :: dash\n",
      "charge.v 370 :: demand\n",
      "civil.a 371 :: thoughtful\n",
      "civil.a 372 :: thoughtful\n",
      "civil.a 373 :: thoughtful\n",
      "civil.a 374 :: public\n",
      "civil.a 375 :: thoughtful\n",
      "civil.a 376 :: thoughtful\n",
      "civil.a 377 :: thoughtful\n",
      "civil.a 378 :: thoughtful\n",
      "civil.a 379 :: thoughtful\n",
      "civil.a 380 :: general\n",
      "clean.v 381 :: wash\n",
      "clean.v 382 :: remedy\n",
      "clean.v 383 :: remedy\n",
      "clean.v 384 :: wipe\n",
      "clean.v 385 :: fix\n",
      "clean.v 386 :: remedy\n",
      "clean.v 387 :: remedy\n",
      "clean.v 388 :: wipe\n",
      "clean.v 389 :: remedy\n",
      "clean.v 390 :: remedy\n",
      "coach.n 391 :: teacher\n",
      "coach.n 392 :: teacher\n",
      "coach.n 393 :: teacher\n",
      "coach.n 394 :: trainer\n",
      "coach.n 395 :: teacher\n",
      "coach.n 396 :: teacher\n",
      "coach.n 397 :: trainer\n",
      "coach.n 398 :: teacher\n",
      "coach.n 399 :: teacher\n",
      "coach.n 400 :: teacher\n",
      "dry.a 401 :: flat\n",
      "dry.a 402 :: dull\n",
      "dry.a 403 :: flat\n",
      "dry.a 404 :: boring\n",
      "dry.a 405 :: flat\n",
      "dry.a 406 :: flat\n",
      "dry.a 407 :: wry\n",
      "dry.a 408 :: flat\n",
      "dry.a 409 :: dull\n",
      "dry.a 410 :: flat\n",
      "full.a 411 :: highest\n",
      "full.a 412 :: highest\n",
      "full.a 413 :: top\n",
      "full.a 414 :: highest\n",
      "full.a 415 :: generous\n",
      "full.a 416 :: highest\n",
      "full.a 417 :: big\n",
      "full.a 418 :: big\n",
      "full.a 419 :: big\n",
      "full.a 420 :: highest\n",
      "heap.n 421 :: category\n",
      "heap.n 422 :: \n",
      "heap.n 423 :: state\n",
      "heap.n 424 :: state\n",
      "heap.n 425 :: lump\n",
      "heap.n 426 :: lump\n",
      "heap.n 427 :: collection\n",
      "heap.n 428 :: lump\n",
      "heap.n 429 :: \n",
      "heap.n 430 :: \n",
      "job.n 431 :: \n",
      "job.n 432 :: matter\n",
      "job.n 433 :: \n",
      "job.n 434 :: \n",
      "job.n 435 :: \n",
      "job.n 436 :: position\n",
      "job.n 437 :: \n",
      "job.n 438 :: \n",
      "job.n 439 :: position\n",
      "job.n 440 :: \n",
      "late.r 441 :: \n",
      "late.r 442 :: \n",
      "late.r 443 :: \n",
      "late.r 444 :: \n",
      "late.r 445 :: \n",
      "late.r 446 :: lately\n",
      "late.r 447 :: behind\n",
      "late.r 448 :: behind\n",
      "late.r 449 :: \n",
      "late.r 450 :: lately\n",
      "new.a 451 :: different\n",
      "new.a 452 :: fresh\n",
      "new.a 453 :: extra\n",
      "new.a 454 :: \n",
      "new.a 455 :: fresh\n",
      "new.a 456 :: extra\n",
      "new.a 457 :: current\n",
      "new.a 458 :: extra\n",
      "new.a 459 :: younger\n",
      "new.a 460 :: extra\n",
      "often.r 461 :: \n",
      "often.r 462 :: \n",
      "often.r 463 :: \n",
      "often.r 464 :: \n",
      "often.r 465 :: \n",
      "often.r 466 :: \n",
      "often.r 467 :: much\n",
      "often.r 468 :: \n",
      "often.r 469 :: \n",
      "often.r 470 :: much\n",
      "only.r 471 :: merely\n",
      "only.r 472 :: just\n",
      "only.r 473 :: hardly\n",
      "only.r 474 :: recently\n",
      "only.r 475 :: hardly\n",
      "only.r 476 :: hardly\n",
      "only.r 477 :: hardly\n",
      "only.r 478 :: recently\n",
      "only.r 479 :: recently\n",
      "only.r 480 :: hardly\n",
      "pulse.n 481 :: energy\n",
      "pulse.n 482 :: heart\n",
      "pulse.n 483 :: stimulus\n",
      "pulse.n 484 :: stimulus\n",
      "pulse.n 485 :: stimulus\n",
      "pulse.n 486 :: energy\n",
      "pulse.n 487 :: \n",
      "pulse.n 488 :: sound\n",
      "pulse.n 489 :: wave\n",
      "pulse.n 490 :: sound\n",
      "put.v 491 :: give\n",
      "put.v 492 :: present\n",
      "put.v 493 :: lay\n",
      "put.v 494 :: give\n",
      "put.v 495 :: give\n",
      "put.v 496 :: get\n",
      "put.v 497 :: present\n",
      "put.v 498 :: present\n",
      "put.v 499 :: give\n",
      "put.v 500 :: advance\n"
     ]
    }
   ],
   "source": [
    "write_file(gensim.models.word2vec.Word2Vec.load('run/word2vec_model'), 'multiplication', 'testtest.txt', vocab_unicode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    part = sys.argv[1]\n",
    "\n",
    "    # this can give you an indication whether part a (vector addition and multiplication) works.\n",
    "    if part == \"a\":\n",
    "        print(\"(a): vector addition and multiplication\")\n",
    "        v1, v2, v3 , v4 = [(0,1), (2,1), (4,2)], [(0,1), (1,2), (4,1)], [1, 0, 1, 0, 2], [1, 2, 0, 0, 1]\n",
    "        try:\n",
    "            if not set(addition(v1, v2)) == set([(0, 2), (2, 1), (4, 3), (1, 2)]):\n",
    "                print(\"\\tError: sparse addition returned wrong result\")\n",
    "            else:\n",
    "                print(\"\\tPass: sparse addition\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: exception raised in sparse addition\")\n",
    "            print(e)\n",
    "        try:\n",
    "            if not set(multiplication(v1, v2)) == set([(0,1), (4,2)]):\n",
    "                print(\"\\tError: sparse multiplication returned wrong result\")\n",
    "            else:\n",
    "                print(\"\\tPass: sparse multiplication\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: exception raised in sparse multiplication\")\n",
    "            print(e)\n",
    "        try:\n",
    "            addition(v3,v4)\n",
    "            print(\"\\tPass: full addition\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: exception raised in full addition\")\n",
    "            print(e)\n",
    "        try:\n",
    "            multiplication(v3,v4)\n",
    "            print(\"\\tPass: full multiplication\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: exception raised in full addition\")\n",
    "            print(e)\n",
    "\n",
    "    # you may complete this to get answers for part b (best substitution words with tf-idf and word2vec, using addition)\n",
    "    if part == \"b\":\n",
    "        print(\"(b) using addition to calculate best substitution words\")\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this to get answers for part c (best substitution words with tf-idf and word2vec, using multiplication)\n",
    "    if part == \"c\":\n",
    "        print(\"(c) using multiplication to calculate best substitution words\")\n",
    "\n",
    "    # this can give you an indication whether your part d1 (P(Z|w) and P(w|Z)) works\n",
    "    if part == \"d\":\n",
    "        print(\"(d): calculating P(Z|w) and P(w|Z)\")\n",
    "        print(\"\\tloading corpus\")\n",
    "        id2word,word2id,vectors=load_corpus(sys.argv[2], sys.argv[3])\n",
    "        print(\"\\tloading LDA model\")\n",
    "        ldaModel = gensim.models.ldamodel.LdaModel.load(\"lda.model\")\n",
    "        houseTopic = ldaModel[vectors[word2id[\"house.n\"]]][0][0]\n",
    "        try:\n",
    "            if prob_z_given_w(ldaModel, houseTopic, vectors[word2id[\"house.n\"]]) > 0.0:\n",
    "                print(\"\\tPass: P(Z|w)\")\n",
    "            else:\n",
    "                print(\"\\tFail: P(Z|w)\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: exception during P(Z|w)\")\n",
    "            print(e)\n",
    "        try:\n",
    "            if prob_w_given_z(ldaModel, \"house.n\", houseTopic) > 0.0:\n",
    "                print(\"\\tPass: P(w|Z)\")\n",
    "            else:\n",
    "                print(\"\\tFail: P(w|Z)\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: exception during P(w|Z)\")\n",
    "            print(e)\n",
    "\n",
    "    # you may complete this to get answers for part d2 (best substitution words with LDA)\n",
    "    if part == \"e\":\n",
    "        print(\"(e): using LDA to calculate best substitution words\")\n",
    "        # your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
