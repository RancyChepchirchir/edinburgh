{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Questions\n",
    "- Length of full vectors assumed to be 5000?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import gensim\n",
    "import math\n",
    "from copy import copy\n",
    "\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1\n",
      "/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1\n"
     ]
    }
   ],
   "source": [
    "# Fix paths\n",
    "import os\n",
    "print os.getcwd()  # '/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1'\n",
    "os.chdir('/Users/sipola/Google Drive/education/coursework/graduate/edinburgh/nlu/nlu-assignment1')\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('run')\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "arg1 = 'a'\n",
    "arg2 = 'data/vocabulary.txt'\n",
    "arg3 = 'data/word_contexts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# these are indices for house, home and time in the data. Don't change.\n",
    "house_noun = 80\n",
    "home_noun = 143\n",
    "time_noun = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(f) helper class, do not modify.\n",
    "provides an iterator over sentences in the provided BNC corpus\n",
    "input: corpus path to the BNC corpus\n",
    "input: n, number of sentences to retrieve (optional, standard -1: all)\n",
    "'''\n",
    "class BncSentences:\n",
    "    def __init__(self, corpus, n=-1):\n",
    "        self.corpus = corpus\n",
    "        self.n = n\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.n\n",
    "        ret = []\n",
    "        for line in open(self.corpus):\n",
    "            line = line.strip().lower()\n",
    "            if line.startswith(\"<s \"):\n",
    "                ret = []\n",
    "            elif line.strip() == \"</s>\":\n",
    "                if n > 0:\n",
    "                    n -= 1\n",
    "                if n == 0:\n",
    "                    break\n",
    "                yield copy(ret)\n",
    "            else:\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == 3:\n",
    "                    word = parts[-1]\n",
    "                    idx = word.rfind(\"-\")\n",
    "                    word, pos = word[:idx], word[idx+1:]\n",
    "                    if word in ['thus', 'late', 'often', 'only', 'usually', 'however', 'lately', 'absolutely', 'hardly', 'fairly', 'near', 'similarly', 'sooner', 'there', 'seriously', 'consequently', 'recently', 'across', 'softly', 'together', 'obviously', 'slightly', 'instantly', 'well', 'therefore', 'solely', 'intimately', 'correctly', 'roughly', 'truly', 'briefly', 'clearly', 'effectively', 'sometimes', 'everywhere', 'somewhat', 'behind', 'heavily', 'indeed', 'sufficiently', 'abruptly', 'narrowly', 'frequently', 'lightly', 'likewise', 'utterly', 'now', 'previously', 'barely', 'seemingly', 'along', 'equally', 'so', 'below', 'apart', 'rather', 'already', 'underneath', 'currently', 'here', 'quite', 'regularly', 'elsewhere', 'today', 'still', 'continuously', 'yet', 'virtually', 'of', 'exclusively', 'right', 'forward', 'properly', 'instead', 'this', 'immediately', 'nowadays', 'around', 'perfectly', 'reasonably', 'much', 'nevertheless', 'intently', 'forth', 'significantly', 'merely', 'repeatedly', 'soon', 'closely', 'shortly', 'accordingly', 'badly', 'formerly', 'alternatively', 'hard', 'hence', 'nearly', 'honestly', 'wholly', 'commonly', 'completely', 'perhaps', 'carefully', 'possibly', 'quietly', 'out', 'really', 'close', 'strongly', 'fiercely', 'strictly', 'jointly', 'earlier', 'round', 'as', 'definitely', 'purely', 'little', 'initially', 'ahead', 'occasionally', 'totally', 'severely', 'maybe', 'evidently', 'before', 'later', 'apparently', 'actually', 'onwards', 'almost', 'tightly', 'practically', 'extremely', 'just', 'accurately', 'entirely', 'faintly', 'away', 'since', 'genuinely', 'neatly', 'directly', 'potentially', 'presently', 'approximately', 'very', 'forwards', 'aside', 'that', 'hitherto', 'beforehand', 'fully', 'firmly', 'generally', 'altogether', 'gently', 'about', 'exceptionally', 'exactly', 'straight', 'on', 'off', 'ever', 'also', 'sharply', 'violently', 'undoubtedly', 'more', 'over', 'quickly', 'plainly', 'necessarily']:\n",
    "                        pos = \"r\"\n",
    "                    if pos == \"j\":\n",
    "                        pos = \"a\"\n",
    "                    ret.append(gensim.utils.any2unicode(word + \".\" + pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "[1, 0, 1, 0, 2]\n",
      "[1, 0, 1, 0, 2]\n",
      "[1, 2, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0]\n",
      "[(0, 1), (2, 1), (4, 2)]\n",
      "[(0, 1), (2, 1), (4, 2)]\n",
      "[(0, 1), (1, 2), (4, 1)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "sparse1 = [(0,1), (2,1), (4,2)]\n",
    "sparse2 = [(0,1), (1,2), (4,1)]\n",
    "sparse3 = []\n",
    "full1 = [1, 0, 1, 0, 2]\n",
    "full2 = [1, 2, 0, 0, 1]\n",
    "full3 = [0, 0, 0, 0, 0]\n",
    "\n",
    "def is_sparse(vector):\n",
    "    return not vector or isinstance(vector[0], tuple)\n",
    "\n",
    "def sparse2full(sparse, length=5000):\n",
    "    sparse = list(sparse)\n",
    "    if not is_sparse(sparse):\n",
    "        return sparse\n",
    "    if length is None:\n",
    "        length = max([tup[0] for tup in sparse]) + 1\n",
    "    full = [0] * length\n",
    "    for tup in sparse:\n",
    "        full[tup[0]] = tup[1]\n",
    "    return full\n",
    "\n",
    "def full2sparse(full):\n",
    "    if is_sparse(full):\n",
    "        return full\n",
    "    sparse = [(c, n) for c, n in enumerate(full) if n > 0]\n",
    "    return sparse\n",
    "\n",
    "print is_sparse(sparse1)\n",
    "print is_sparse(sparse3)\n",
    "print is_sparse(full1)\n",
    "\n",
    "print sparse2full(sparse1, 5)\n",
    "print sparse2full(full1, 5)\n",
    "print sparse2full(sparse2, 5)\n",
    "print sparse2full(sparse3, 5)\n",
    "\n",
    "print full2sparse(full1)\n",
    "print full2sparse(sparse1)\n",
    "print full2sparse(full2)\n",
    "print full2sparse(full3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(a) function load_corpus to read a corpus from disk\n",
    "input: vocabFile containing vocabulary\n",
    "input: contextFile containing word contexts\n",
    "output: id2word mapping word IDs to words\n",
    "output: word2id mapping words to word IDs\n",
    "output: vectors for the corpus, as a list of sparse vectors\n",
    "'''\n",
    "def load_corpus(vocabFile, contextFile):\n",
    "    id2word = {}\n",
    "    word2id = {}\n",
    "    vectors = []\n",
    "\n",
    "    with open(vocabFile, 'r') as f:\n",
    "        vocab = f.read().splitlines()\n",
    "    \n",
    "    with open(contextFile, 'r') as f:\n",
    "        context = f.read().splitlines()\n",
    "    \n",
    "    for big_string in context:\n",
    "        tups = []\n",
    "        if big_string[0] != '0':  # words without context words have a string that starts with '0'\n",
    "            cns = big_string.split(' ')[1:]\n",
    "            for cn in cns:\n",
    "                c, n = cn.split(':')\n",
    "                tups.append((int(c), int(n)))\n",
    "        vectors.append(tups)\n",
    "    \n",
    "    for ID, word in enumerate(vocab):\n",
    "        id2word[ID] = word\n",
    "        word2id[word] = ID\n",
    "\n",
    "    return id2word, word2id, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a): load corpus\n",
      "\tPass: load corpus from file\n",
      "\tPass: id2word\n",
      "\tPass: word2id\n"
     ]
    }
   ],
   "source": [
    "# this can give you an indication whether part a (loading a corpus) works.\n",
    "# not guaranteed that everything works.\n",
    "print(\"(a): load corpus\")\n",
    "try:\n",
    "    id2word, word2id, vectors = load_corpus(arg2, arg3)\n",
    "    if not id2word:\n",
    "        print(\"\\tError: id2word is None or empty\")\n",
    "        exit()\n",
    "    if not word2id:\n",
    "        print(\"\\tError: id2word is None or empty\")\n",
    "        exit()\n",
    "    if not vectors:\n",
    "        print(\"\\tError: id2word is None or empty\")\n",
    "        exit()\n",
    "    print(\"\\tPass: load corpus from file\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: could not load corpus from disk\")\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    if not id2word[house_noun] == \"house.n\" or not id2word[home_noun] == \"home.n\" or not id2word[time_noun] == \"time.n\":\n",
    "        print(\"\\tError: id2word fails to retrive correct words for ids\")\n",
    "    else:\n",
    "        print(\"\\tPass: id2word\")\n",
    "except Exception:\n",
    "    print(\"\\tError: Exception in id2word\")\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    if not word2id[\"house.n\"] == house_noun or not word2id[\"home.n\"] == home_noun or not word2id[\"time.n\"] == time_noun:\n",
    "        print(\"\\tError: word2id fails to retrive correct ids for words\")\n",
    "    else:\n",
    "        print(\"\\tPass: word2id\")\n",
    "except Exception:\n",
    "    print(\"\\tError: Exception in word2id\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "id2word, word2id, vectors = load_corpus('data/vocabulary.txt', 'data/word_contexts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(b) function cosine_similarity to calculate similarity between 2 vectors\n",
    "input: vector1\n",
    "input: vector2\n",
    "output: cosine similarity between vector1 and vector2 as a real number\n",
    "'''\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \n",
    "    # Make sure inputs are full.\n",
    "    vector1 = sparse2full(vector1)\n",
    "    vector2 = sparse2full(vector2)\n",
    "    \n",
    "    def dot_prod(v1, v2):\n",
    "        return sum([x * y for x, y in zip(v1, v2)])\n",
    "\n",
    "    num = dot_prod(vector1, vector2)\n",
    "    denom = dot_prod(vector1, vector1)**0.5 * dot_prod(vector2, vector2)**0.5\n",
    "    \n",
    "    return num / float(denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1, 0], [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b): cosine similarity\n",
      "\tPass: sparse vector similarity\n",
      "\tPass: full vector similarity\n"
     ]
    }
   ],
   "source": [
    "# this can give you an indication whether part b (cosine similarity) works.\n",
    "# these are very simple dummy vectors, no guarantee it works for our actual vectors.\n",
    "import numpy\n",
    "print(\"(b): cosine similarity\")\n",
    "try:\n",
    "    cos = cosine_similarity([(0,1), (2,1), (4,2)], [(0,1), (1,2), (4,1)])\n",
    "    if not numpy.isclose(0.5, cos):\n",
    "        print(\"\\tError: sparse expected similarity is 0.5, was {0}\".format(cos))\n",
    "    else:\n",
    "        print(\"\\tPass: sparse vector similarity\")\n",
    "except Exception:\n",
    "    print(\"\\tError: failed for sparse vector\")\n",
    "try:\n",
    "    cos = cosine_similarity([1, 0, 1, 0, 2], [1, 2, 0, 0, 1])\n",
    "    if not numpy.isclose(0.5, cos):\n",
    "        print(\"\\tError: full expected similarity is 0.5, was {0}\".format(cos))\n",
    "    else:\n",
    "        print(\"\\tPass: full vector similarity\")\n",
    "except Exception:\n",
    "    print(\"\\tError: failed for full vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_test_word_list():\n",
    "    w1 = (80, 'house.n')\n",
    "    w2 = (143, 'home.n')\n",
    "    w3 = (12, 'time.n')\n",
    "    return [w1, w2, w3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_words(vectors, words_to_test = make_test_word_list()):\n",
    "    for i, tup1 in enumerate(words_to_test):\n",
    "        for j, tup2 in enumerate(words_to_test):\n",
    "            if j > i:\n",
    "                id1, str1 = tup1\n",
    "                id2, str2 = tup2\n",
    "                sim = cosine_similarity(vectors[id1], vectors[id2])\n",
    "                print 'similarity between {} and {}: {}'.format(str1, str2, sim)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between house.n and home.n: 0.812743856464\n",
      "similarity between house.n and time.n: 0.82359219053\n",
      "similarity between home.n and time.n: 0.818144793373\n"
     ]
    }
   ],
   "source": [
    "test_words(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(d) function tf_idf to turn existing frequency-based vector model into tf-idf-based vector model\n",
    "input: freqVectors, a list of frequency-based vectors\n",
    "output: tfIdfVectors, a list of tf-idf-based vectors\n",
    "'''\n",
    "def tf_idf(freqVectors):\n",
    "    \n",
    "    tfIdfVectors = []\n",
    "    N = len(freqVectors)\n",
    "    \n",
    "    # Convert to sparse vectors.\n",
    "    freqVectors = [full2sparse(fv) for fv in freqVectors]\n",
    "    \n",
    "    # Define function that calculates TF-IDF for a single term.\n",
    "    def tf_idf_word(tf, N, df):\n",
    "        return (1 + math.log(tf, 2)) * (1 + math.log(N / float(df), 2))\n",
    "\n",
    "    # Generate dictionary df (document frequency of term). Do this by iterating over documents.\n",
    "    df_dict = {}\n",
    "    for fv in freqVectors:\n",
    "        for term, _ in fv:  # don't need frequency\n",
    "            df_dict[term] = df_dict.get(term, 0) + 1\n",
    "        \n",
    "    for fv in freqVectors:\n",
    "        tfIdfVectors.append([(term, tf_idf_word(tf, N, df_dict[term])) for term, tf in fv])\n",
    "\n",
    "    return tfIdfVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 1, 0, 2], [(0, 1), (1, 2), (4, 1)], [(0, 1), (2, 1), (4, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 1.0), (2, 1.584962500721156), (4, 2.0)],\n",
       " [(0, 1.0), (1, 5.169925001442312), (4, 1.0)],\n",
       " [(0, 1.0), (2, 1.584962500721156), (4, 1.0)]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse1 = [(0,1), (2,1), (4,2)]\n",
    "sparse2 = [(0,1), (1,2), (4,1)]\n",
    "sparse3 = [(0,1), (2,1), (4,1)]\n",
    "full1 = [1, 0, 1, 0, 2]\n",
    "full2 = [1, 2, 0, 0, 1]\n",
    "\n",
    "vectors_test = [full1, sparse2, sparse3]\n",
    "\n",
    "# for i, fv in enumerate(vectors_test):\n",
    "#     if not is_sparse(fv):\n",
    "#         vectors_test[i] = full2sparse(fv)\n",
    "\n",
    "print vectors_test\n",
    "\n",
    "tf_idf(vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 46.447381754818345), (7, 60.42345848148703), (4, 48.012182361527785), (3, 47.77588941708801), (6, 54.736678364323744), (20, 55.45621663800088), (2, 43.09843681189252), (8, 47.77353357204966), (10, 53.73375562076166), (9, 53.766808961711504)]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_output = tf_idf(vectors)\n",
    "print tf_idf_output[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between house.n and home.n: 0.644002976484\n",
      "similarity between house.n and time.n: 0.57753821538\n",
      "similarity between home.n and time.n: 0.54386864522\n"
     ]
    }
   ],
   "source": [
    "test_words(tf_idf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d) converting to tf-idf space\n",
      "\tPass: converted to tf-idf space\n"
     ]
    }
   ],
   "source": [
    "# this gives you an indication whether your conversion into tf-idf space works.\n",
    "# this does not test for vector values in tf-idf space, hence can't tell you whether tf-idf has been implemented correctly\n",
    "print(\"(d) converting to tf-idf space\")\n",
    "id2word, word2id, vectors = load_corpus(arg2, arg3)\n",
    "try:\n",
    "    tfIdfSpace = tf_idf(vectors)\n",
    "    if not len(vectors) == len(tfIdfSpace):\n",
    "        print(\"\\tError: tf-idf space does not correspond to original vector space\")\n",
    "    else:\n",
    "        print(\"\\tPass: converted to tf-idf space\")\n",
    "except Exception as e:\n",
    "    print(\"\\tError: could not convert to tf-idf space\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(f) function word2vec to build a word2vec vector model with 100 dimensions and window size 5\n",
    "'''\n",
    "def word2vec(corpus, learningRate, downsampleRate, negSampling):\n",
    "    \n",
    "    model = gensim.models.word2vec.Word2Vec(sentences=corpus,\n",
    "                                            alpha=learningRate,\n",
    "                                            sample=downsampleRate,\n",
    "                                            negative=negSampling,\n",
    "                                            size=100,\n",
    "                                            window=5)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learningRate = 0.03\n",
    "downsampleRate = 0.001\n",
    "negSampling = 5\n",
    "\n",
    "model = word2vec(BncSentences('/Users/sipola/Desktop/bnc.vert', 50000),\n",
    "                 learningRate,\n",
    "                 downsampleRate,\n",
    "                 negSampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate 0.01, downsampleRate 0.00000, negSampling  0, time                                                                                        89.3002929688, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.00000, negSampling  5, time                                                                                        88.3039929867, acc 0.0031780\n",
      "learningRate 0.01, downsampleRate 0.00000, negSampling 10, time                                                                                        88.8333699703, acc 0.0026483\n",
      "learningRate 0.01, downsampleRate 0.00001, negSampling  0, time                                                                                        88.6777968407, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.00001, negSampling  5, time                                                                                        83.3220889568, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.00001, negSampling 10, time                                                                                        88.7574248314, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.00100, negSampling  0, time                                                                                        83.5120699406, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.00100, negSampling  5, time                                                                                        78.3804681301, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.00100, negSampling 10, time                                                                                        79.0222070217, acc 0.0026483\n",
      "learningRate 0.01, downsampleRate 0.10000, negSampling  0, time                                                                                        88.8876328468, acc 0.0000000\n",
      "learningRate 0.01, downsampleRate 0.10000, negSampling  5, time                                                                                        82.3662281036, acc 0.0031780\n",
      "learningRate 0.01, downsampleRate 0.10000, negSampling 10, time                                                                                        82.3089489937, acc 0.0047669\n",
      "learningRate 0.03, downsampleRate 0.00000, negSampling  0, time                                                                                        82.5470449924, acc 0.0000000\n",
      "learningRate 0.03, downsampleRate 0.00000, negSampling  5, time                                                                                        85.3491039276, acc 0.0079449\n",
      "learningRate 0.03, downsampleRate 0.00000, negSampling 10, time                                                                                        86.3479418755, acc 0.0132415\n",
      "learningRate 0.03, downsampleRate 0.00001, negSampling  0, time                                                                                        79.2825889587, acc 0.0000000\n",
      "learningRate 0.03, downsampleRate 0.00001, negSampling  5, time                                                                                        82.4482581615, acc 0.0000000\n",
      "learningRate 0.03, downsampleRate 0.00001, negSampling 10, time                                                                                        85.8922281265, acc 0.0000000\n",
      "learningRate 0.03, downsampleRate 0.00100, negSampling  0, time                                                                                        83.7036290169, acc 0.0000000\n",
      "learningRate 0.03, downsampleRate 0.00100, negSampling  5, time                                                                                        88.9615051746, acc 0.0116525\n",
      "learningRate 0.03, downsampleRate 0.00100, negSampling 10, time                                                                                        85.7641069889, acc 0.0153602\n",
      "learningRate 0.03, downsampleRate 0.10000, negSampling  0, time                                                                                         87.127477169, acc 0.0000000\n",
      "learningRate 0.03, downsampleRate 0.10000, negSampling  5, time                                                                                        87.9297840595, acc 0.0116525\n",
      "learningRate 0.03, downsampleRate 0.10000, negSampling 10, time                                                                                        85.3343729973, acc 0.0169492\n",
      "learningRate 0.05, downsampleRate 0.00000, negSampling  0, time                                                                                        81.3252079487, acc 0.0000000\n",
      "learningRate 0.05, downsampleRate 0.00000, negSampling  5, time                                                                                        85.9110250473, acc 0.0211864\n",
      "learningRate 0.05, downsampleRate 0.00000, negSampling 10, time                                                                                        80.2153000832, acc 0.0254237\n",
      "learningRate 0.05, downsampleRate 0.00001, negSampling  0, time                                                                                        97.0665011406, acc 0.0000000\n",
      "learningRate 0.05, downsampleRate 0.00001, negSampling  5, time                                                                                        86.8441300392, acc 0.0000000\n",
      "learningRate 0.05, downsampleRate 0.00001, negSampling 10, time                                                                                        87.0752091408, acc 0.0000000\n",
      "learningRate 0.05, downsampleRate 0.00100, negSampling  0, time                                                                                        86.6025998592, acc 0.0000000\n",
      "learningRate 0.05, downsampleRate 0.00100, negSampling  5, time                                                                                         89.295345068, acc 0.0280720\n",
      "learningRate 0.05, downsampleRate 0.00100, negSampling 10, time                                                                                        88.7648389339, acc 0.0370763\n",
      "learningRate 0.05, downsampleRate 0.10000, negSampling  0, time                                                                                        87.8686611652, acc 0.0000000\n",
      "learningRate 0.05, downsampleRate 0.10000, negSampling  5, time                                                                                        93.2319531441, acc 0.0264831\n",
      "learningRate 0.05, downsampleRate 0.10000, negSampling 10, time                                                                                         91.193611145, acc 0.0238347\n"
     ]
    }
   ],
   "source": [
    "learningRate_list = [0.01, 0.03, 0.05]\n",
    "downsampleRate_list = [0., 0.00001, 0.001, 0.1]\n",
    "negSampling_list = [0, 5, 10]\n",
    "\n",
    "acc_list = []\n",
    "\n",
    "for learningRate in learningRate_list:\n",
    "    for downsampleRate in downsampleRate_list:\n",
    "        for negSampling in negSampling_list:\n",
    "            \n",
    "            t0 = time.time()\n",
    "            model = word2vec(BncSentences('/Users/sipola/Desktop/bnc.vert', 50000),\n",
    "                             learningRate,\n",
    "                             downsampleRate,\n",
    "                             negSampling)\n",
    "            t1 = time.time()\n",
    "            \n",
    "            acc_one = model.accuracy('accuracy_test.txt')\n",
    "            right = len(acc_one[-1]['correct'])\n",
    "            wrong = len(acc_one[-1]['incorrect'])\n",
    "            acc = right / float(right + wrong)\n",
    "            \n",
    "            acc_list.append(acc)\n",
    "\n",
    "            print 'learningRate {:.2f}, downsampleRate {:.5f}, negSampling {:2}, time {:4}, acc {:.7f}'.format(learningRate, downsampleRate, negSampling, t1-t0, acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(acc_list, open(os.path.join(\"run\", \"word2vec_acc_list.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "learningRate = 0.05\n",
    "downsampleRate = 0.001\n",
    "negSampling = 10\n",
    "\n",
    "t0 = time.time()\n",
    "model = word2vec(BncSentences('/Users/sipola/Desktop/bnc.vert'),\n",
    "                 learningRate,\n",
    "                 downsampleRate,\n",
    "                 negSampling)\n",
    "t1 = time.time()\n",
    "\n",
    "# Run next section to save model and print stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learningRate 0.05, downsampleRate 0.00100, negSampling 10, time                                                                                         13186.149436, acc 0.4299133\n"
     ]
    }
   ],
   "source": [
    "# Save model!\n",
    "model.save('run/word2vec_model')\n",
    "pickle.dump(model, open(os.path.join(\"run\", \"word2vec_model_pickle.p\"), \"wb\"))\n",
    "\n",
    "acc_dict = model.accuracy('data/accuracy_test.txt')\n",
    "right = len(acc_dict[-1]['correct'])\n",
    "wrong = len(acc_dict[-1]['incorrect'])\n",
    "acc = right / float(right + wrong)            \n",
    "\n",
    "print 'learningRate {:.2f}, downsampleRate {:.5f}, negSampling {:2}, time {:100}, acc {:.7f}'.format(learningRate, downsampleRate, negSampling, t1-t0, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between house.n and home.n: 0.575759368328\n",
      "similarity between house.n and time.n: 0.191773125161\n",
      "similarity between home.n and time.n: 0.380011934183\n"
     ]
    }
   ],
   "source": [
    "words_to_test = ['house.n', 'home.n', 'time.n']\n",
    "for i, w1 in enumerate(words_to_test):\n",
    "    for j, w2 in enumerate(words_to_test):\n",
    "        if j > i:\n",
    "            sim = model.similarity(w1, w2)\n",
    "            print 'similarity between {} and {}: {}'.format(w1, w2, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(f2) word2vec, building full model with best parameters. May take a while.\n"
     ]
    }
   ],
   "source": [
    "# you may complete this part for the second part of f (training and saving the actual word2vec model)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "print(\"(f2) word2vec, building full model with best parameters. May take a while.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(h) function lda to build an LDA model with 100 topics from a frequency vector space\n",
    "input: vectors\n",
    "input: wordMapping mapping from word IDs to words\n",
    "output: an LDA topic model with 100 topics, using the frequency vectors\n",
    "'''\n",
    "def lda(vectors, wordMapping):\n",
    "    model = gensim.models.ldamodel.LdaModel(vectors, id2word=wordMapping, num_topics=100, passes=10, update_every=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_lda = lda(vectors, id2word)\n",
    "model_lda.save('run/lda_model')\n",
    "pickle.dump(model_lda, open(os.path.join(\"run\", \"lda_model_pickle.p\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity between house.n and home.n: 0.428085707761\n",
      "similarity between house.n and time.n: 0.0580350221231\n",
      "similarity between home.n and time.n: 0.348057003822\n"
     ]
    }
   ],
   "source": [
    "words_to_test = ['house.n', 'home.n', 'time.n']\n",
    "for i, w1 in enumerate(words_to_test):\n",
    "    for j, w2 in enumerate(words_to_test):\n",
    "        if j > i:\n",
    "            v1 = model_lda[vectors[word2id[w1]]]\n",
    "            v2 = model_lda[vectors[word2id[w2]]]\n",
    "            sim = cosine_similarity(v1, v2)\n",
    "            print 'similarity between {} and {}: {}'.format(w1, w2, sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "(j) function get_topic_words, to get words in a given LDA topic\n",
    "input: ldaModel, pre-trained Gensim LDA model\n",
    "input: topicID, ID of the topic for which to get topic words\n",
    "input: wordMapping, mapping from words to IDs (optional)\n",
    "'''\n",
    "def get_topic_words(ldaModel, topicID, topn=10):\n",
    "    return ldaModel.show_topic(topicID, topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: take, for, have, will, can, do, would, where, no, see\n",
      "Topic 1: have, can, will, do, may, would, also, should, other, could\n",
      "Topic 2: do, have, make, get, see, take, no, think, also, use\n",
      "Topic 3: human, have, do, would, get, will, make, say, can, know\n",
      "Topic 4: have, cent, do, sexual, can, will, child, no, out, would\n",
      "Topic 5: more, have, very, most, so, make, become, less, will, would\n",
      "Topic 6: have, president, de, attack, do, would, pool, say, go, get\n",
      "Topic 7: time, dr, car, have, j, m, will, professor, nation, to\n",
      "Topic 8: have, do, well, would, say, good, far, think, can, get\n",
      "Topic 9: have, do, can, would, make, will, get, take, time, go\n",
      "Topic 10: have, would, do, will, force, up, time, can, say, could\n",
      "Topic 11: have, south, north, west, east, country, england, go, road, other\n",
      "Topic 12: have, service, health, people, child, local, authority, council, member, care\n",
      "Topic 13: have, will, gas, would, do, make, say, can, go, company\n",
      "Topic 14: united, have, nuclear, do, new, will, blood, waste, would, use\n",
      "Topic 15: have, no, do, would, will, say, give, can, take, could\n",
      "Topic 16: say, have, ask, do, police, officer, would, executive, answer, will\n",
      "Topic 17: have, cause, house, do, suffer, would, will, can, injury, could\n",
      "Topic 18: have, white, do, black, would, look, wear, make, get, go\n",
      "Topic 19: have, do, will, say, would, up, go, can, get, new\n",
      "Topic 20: have, make, will, do, would, can, take, may, positive, use\n",
      "Topic 21: have, system, date, ago, will, new, later, begin, end, year\n",
      "Topic 22: have, do, concentration, would, will, say, party, public, can, poll\n",
      "Topic 23: have, do, would, get, will, say, can, so, go, know\n",
      "Topic 24: have, go, do, and, will, so, get, would, can, say\n",
      "Topic 25: have, go, do, get, say, will, great, sentence, more, can\n",
      "Topic 26: have, cell, than, large, small, high, can, will, do, use\n",
      "Topic 27: minister, have, secretary, affair, will, prime, ministry, government, foreign, deputy\n",
      "Topic 28: have, can, do, would, will, could, time, make, may, only\n",
      "Topic 29: have, per, will, rate, would, pay, price, cost, tax, can\n",
      "Topic 30: have, do, take, will, would, can, make, say, get, up\n",
      "Topic 31: have, order, come, particular, get, do, addition, general, go, will\n",
      "Topic 32: have, can, will, could, p, do, would, also, how, word\n",
      "Topic 33: have, do, will, write, can, use, get, book, make, would\n",
      "Topic 34: have, do, go, file, will, can, would, new, use, say\n",
      "Topic 35: in, have, do, will, would, can, make, time, out, say\n",
      "Topic 36: have, do, will, can, so, hand, people, thing, make, get\n",
      "Topic 37: get, do, would, see, say, no, go, make, so, take\n",
      "Topic 38: have, do, will, would, can, more, get, take, make, say\n",
      "Topic 39: have, up, down, out, back, then, will, off, do, can\n",
      "Topic 40: have, old, young, award, man, woman, year, do, no, other\n",
      "Topic 41: new, have, machine, will, do, would, reaction, chain, make, up\n",
      "Topic 42: have, force, do, will, can, could, would, may, no, out\n",
      "Topic 43: have, do, other, make, will, get, go, would, say, can\n",
      "Topic 44: have, say, do, mr, would, will, go, john, get, take\n",
      "Topic 45: have, do, would, will, make, get, see, say, go, take\n",
      "Topic 46: have, cup, world, make, room, do, will, tea, would, go\n",
      "Topic 47: have, will, use, can, system, new, other, provide, information, make\n",
      "Topic 48: of, have, school, do, drug, no, will, would, say, know\n",
      "Topic 49: have, tree, man, will, can, other, do, make, plant, up\n",
      "Topic 50: that, have, age, do, way, year, would, make, to, will\n",
      "Topic 51: have, ref, will, do, can, now, would, year, no, make\n",
      "Topic 52: have, new, will, committee, government, would, make, report, do, also\n",
      "Topic 53: have, carry, go, do, point, find, come, will, get, would\n",
      "Topic 54: have, know, think, say, so, want, how, go, get, like\n",
      "Topic 55: have, make, will, do, would, can, should, government, may, new\n",
      "Topic 56: have, water, will, can, air, energy, do, light, time, get\n",
      "Topic 57: eye, have, do, will, mr, would, say, bill, see, make\n",
      "Topic 58: have, will, do, say, per, go, space, would, car, get\n",
      "Topic 59: do, have, out, will, say, would, can, get, go, know\n",
      "Topic 60: least, all, have, once, do, last, first, no, will, would\n",
      "Topic 61: have, head, do, say, up, other, will, voice, would, can\n",
      "Topic 62: well, have, possible, far, can, will, just, do, so, go\n",
      "Topic 63: have, london, house, will, st, city, go, university, street, new\n",
      "Topic 64: have, give, see, show, can, will, take, use, make, do\n",
      "Topic 65: have, northern, will, do, would, can, go, france, make, germany\n",
      "Topic 66: have, important, will, war, can, would, do, become, difficult, much\n",
      "Topic 67: have, place, inc, will, do, would, time, over, can, part\n",
      "Topic 68: have, number, do, very, will, make, animal, can, up, other\n",
      "Topic 69: at, have, right, do, say, no, again, will, get, would\n",
      "Topic 70: have, company, british, will, union, trade, would, other, new, industry\n",
      "Topic 71: have, problem, will, can, would, do, could, issue, question, difficulty\n",
      "Topic 72: have, do, as, make, would, how, so, more, can, will\n",
      "Topic 73: year, cent, have, about, month, time, only, will, week, would\n",
      "Topic 74: have, do, say, think, go, so, get, well, would, know\n",
      "Topic 75: have, party, labour, government, would, political, election, member, leader, national\n",
      "Topic 76: have, do, get, make, go, will, egg, say, would, can\n",
      "Topic 77: have, time, period, is, world, do, term, will, would, take\n",
      "Topic 78: have, can, will, do, would, could, so, say, out, see\n",
      "Topic 79: have, director, say, do, would, will, see, can, up, more\n",
      "Topic 80: example, instance, have, up, ever, do, can, will, long, would\n",
      "Topic 81: course, have, do, sort, will, say, would, can, go, know\n",
      "Topic 82: have, longer, way, make, will, do, would, can, say, could\n",
      "Topic 83: have, day, will, time, go, year, night, do, take, play\n",
      "Topic 84: hon, to, have, from, right, will, do, family, make, say\n",
      "Topic 85: have, set, pick, make, will, get, do, would, can, come\n",
      "Topic 86: have, do, j, say, up, would, see, a, will, m\n",
      "Topic 87: have, go, get, come, take, then, put, will, look, do\n",
      "Topic 88: have, social, will, economic, would, make, can, give, government, political\n",
      "Topic 89: have, friend, say, will, do, member, gentleman, can, right, would\n",
      "Topic 90: have, corp, system, will, do, year, can, co, software, use\n",
      "Topic 91: have, do, social, school, education, art, course, study, science, year\n",
      "Topic 92: have, will, do, can, would, up, no, time, should, could\n",
      "Topic 93: have, will, area, would, make, act, do, public, law, new\n",
      "Topic 94: et, have, view, out, will, do, make, say, get, would\n",
      "Topic 95: have, court, will, would, say, prime, state, do, mr, new\n",
      "Topic 96: have, out, on, will, so, use, can, do, up, car\n",
      "Topic 97: have, see, will, year, hold, meeting, would, government, national, say\n",
      "Topic 98: have, group, class, will, other, area, can, would, ireland, people\n",
      "Topic 99: have, will, question, do, would, can, go, see, get, make\n"
     ]
    }
   ],
   "source": [
    "# you may complete this part to get answers for part j (topic words in your LDA model)\n",
    "for topic_id in range(100):\n",
    "    wp = get_topic_words(model_lda, topic_id)\n",
    "    word_str = ', '.join([re.sub('\\..*', '', tup[0]) for tup in wp])\n",
    "    print 'Topic {}: {}'.format(topic_id, word_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "\n",
    "    part = sys.argv[1].lower()\n",
    "\n",
    "    # these are indices for house, home and time in the data. Don't change.\n",
    "    house_noun = 80\n",
    "    home_noun = 143\n",
    "    time_noun = 12\n",
    "\n",
    "    # this can give you an indication whether part a (loading a corpus) works.\n",
    "    # not guaranteed that everything works.\n",
    "    if part == \"a\":\n",
    "        print(\"(a): load corpus\")\n",
    "        try:\n",
    "            id2word, word2id, vectors = load_corpus(sys.argv[2], sys.argv[3])\n",
    "            if not id2word:\n",
    "                print(\"\\tError: id2word is None or empty\")\n",
    "                exit()\n",
    "            if not word2id:\n",
    "                print(\"\\tError: id2word is None or empty\")\n",
    "                exit()\n",
    "            if not vectors:\n",
    "                print(\"\\tError: id2word is None or empty\")\n",
    "                exit()\n",
    "            print(\"\\tPass: load corpus from file\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: could not load corpus from disk\")\n",
    "            print(e)\n",
    "\n",
    "        try:\n",
    "            if not id2word[house_noun] == \"house.n\" or not id2word[home_noun] == \"home.n\" or not id2word[time_noun] == \"time.n\":\n",
    "                print(\"\\tError: id2word fails to retrive correct words for ids\")\n",
    "            else:\n",
    "                print(\"\\tPass: id2word\")\n",
    "        except Exception:\n",
    "            print(\"\\tError: Exception in id2word\")\n",
    "            print(e)\n",
    "\n",
    "        try:\n",
    "            if not word2id[\"house.n\"] == house_noun or not word2id[\"home.n\"] == home_noun or not word2id[\"time.n\"] == time_noun:\n",
    "                print(\"\\tError: word2id fails to retrive correct ids for words\")\n",
    "            else:\n",
    "                print(\"\\tPass: word2id\")\n",
    "        except Exception:\n",
    "            print(\"\\tError: Exception in word2id\")\n",
    "            print(e)\n",
    "\n",
    "    # this can give you an indication whether part b (cosine similarity) works.\n",
    "    # these are very simple dummy vectors, no guarantee it works for our actual vectors.\n",
    "    if part == \"b\":\n",
    "        import numpy\n",
    "        print(\"(b): cosine similarity\")\n",
    "        try:\n",
    "            cos = cosine_similarity([(0,1), (2,1), (4,2)], [(0,1), (1,2), (4,1)])\n",
    "            if not numpy.isclose(0.5, cos):\n",
    "                print(\"\\tError: sparse expected similarity is 0.5, was {0}\".format(cos))\n",
    "            else:\n",
    "                print(\"\\tPass: sparse vector similarity\")\n",
    "        except Exception:\n",
    "            print(\"\\tError: failed for sparse vector\")\n",
    "        try:\n",
    "            cos = cosine_similarity([1, 0, 1, 0, 2], [1, 2, 0, 0, 1])\n",
    "            if not numpy.isclose(0.5, cos):\n",
    "                print(\"\\tError: full expected similarity is 0.5, was {0}\".format(cos))\n",
    "            else:\n",
    "                print(\"\\tPass: full vector similarity\")\n",
    "        except Exception:\n",
    "            print(\"\\tError: failed for full vector\")\n",
    "\n",
    "    # you may complete this part to get answers for part c (similarity in frequency space)\n",
    "    if part == \"c\":\n",
    "        print(\"(c) similarity of house, home and time in frequency space\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # this gives you an indication whether your conversion into tf-idf space works.\n",
    "    # this does not test for vector values in tf-idf space, hence can't tell you whether tf-idf has been implemented correctly\n",
    "    if part == \"d\":\n",
    "        print(\"(d) converting to tf-idf space\")\n",
    "        id2word, word2id, vectors = load_corpus(sys.argv[2], sys.argv[3])\n",
    "        try:\n",
    "            tfIdfSpace = tf_idf(vectors)\n",
    "            if not len(vectors) == len(tfIdfSpace):\n",
    "                print(\"\\tError: tf-idf space does not correspond to original vector space\")\n",
    "            else:\n",
    "                print(\"\\tPass: converted to tf-idf space\")\n",
    "        except Exception as e:\n",
    "            print(\"\\tError: could not convert to tf-idf space\")\n",
    "            print(e)\n",
    "\n",
    "    # you may complete this part to get answers for part e (similarity in tf-idf space)\n",
    "    if part == \"e\":\n",
    "        print(\"(e) similarity of house, home and time in tf-idf space\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this part for the first part of f (estimating best learning rate, sample rate and negative samplings)\n",
    "    if part == \"f1\":\n",
    "        print(\"(f1) word2vec, estimating best learning rate, sample rate, negative sampling\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this part for the second part of f (training and saving the actual word2vec model)\n",
    "    if part == \"f2\":\n",
    "        import logging\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        print(\"(f2) word2vec, building full model with best parameters. May take a while.\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this part to get answers for part g (similarity in your word2vec model)\n",
    "    if part == \"g\":\n",
    "        print(\"(g): word2vec based similarity\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this for part h (training and saving the LDA model)\n",
    "    if part == \"h\":\n",
    "        import logging\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "        print(\"(h) LDA model\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this part to get answers for part i (similarity in your LDA model)\n",
    "    if part == \"i\":\n",
    "        print(\"(i): lda-based similarity\")\n",
    "\n",
    "        # your code here\n",
    "\n",
    "    # you may complete this part to get answers for part j (topic words in your LDA model)\n",
    "    if part == \"j\":\n",
    "        print(\"(j) get topics from LDA model\")\n",
    "\n",
    "        # your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
