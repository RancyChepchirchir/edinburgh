{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "\n",
    "# Import the gutenberg corpus\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Import NLTK's NgramModel module (for building language models)\n",
    "# It has been removed from standard NLTK, so we access it in a special package installation\n",
    "sys.path.extend(['/group/ltg/projects/fnlp', '/group/ltg/projects/fnlp/packages_2.6'])\n",
    "from nltkx import NgramModel\n",
    "\n",
    "# Import probability distributions\n",
    "from nltk.probability import LaplaceProbDist\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.probability import SimpleGoodTuringProbDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE:\n",
      "Probability of 'end' given 'the': 0.00584652862363\n",
      "Probability of 'the' given 'end': 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#################### EXERCISE 0 ####################\n",
    "\n",
    "# Solution for exercise 0\n",
    "# Input: word (string), context (string)\n",
    "# Output: p (float)\n",
    "# Compute the unsmoothed (MLE) probability for word given the single word context\n",
    "def ex0(word,context):\n",
    "    p = 0.0\n",
    "    \n",
    "    word = word.lower()\n",
    "    context = context.lower()\n",
    "    \n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "    austen_bigrams = zip(austen_words[:-1], austen_words[1:])  # list of bigrams as tuples\n",
    "    # (above doesn't include begin/end of corpus: but basically this is fine)\n",
    "\n",
    "    # Compute probability of word given context. Make sure you use float division.\n",
    "    num = np.sum([tup == (context, word) for tup in austen_bigrams])\n",
    "    denom = np.sum([tup[0] == context for tup in austen_bigrams])\n",
    "    p = num / float(denom)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n",
    "\n",
    "### Uncomment to test exercise 0\n",
    "print \"MLE:\"\n",
    "result0a = ex0('end','the')\n",
    "print \"Probability of \\'end\\' given \\'the\\': \" + str(result0a)\n",
    "result0b = ex0('the','end')\n",
    "print \"Probability of \\'the\\' given \\'end\\': \" + str(result0b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAPLACE:\n",
      "Probability of 'end' given 'the': 0.00237913970308\n",
      "Probability of 'the' given 'end': 0.00015487068298\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 1 ####################\n",
    "\n",
    "# Solution for exercise 1\n",
    "# Input: word (string), context (string)\n",
    "# Output: p (float)\n",
    "# Compute the Laplace smoothed probability for word given the single word context\n",
    "def ex1(word,context):\n",
    "    p = 0.0\n",
    "    \n",
    "    word = word.lower()\n",
    "    context = context.lower()\n",
    "    \n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "    austen_bigrams = zip(austen_words[:-1], austen_words[1:])  # list of bigrams as tuples\n",
    "    # (above doesn't include begin/end of corpus: but basically this is fine)\n",
    "    \n",
    "    # compute the vocabulary size\n",
    "    V = len(set(austen_words))\n",
    "\n",
    "    # Compute probability of word given context\n",
    "    num = np.sum([tup == (context, word) for tup in austen_bigrams]) + 1\n",
    "    denom = np.sum([tup[0] == context for tup in austen_bigrams]) + V\n",
    "    p = num / float(denom)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 1\n",
    "print \"LAPLACE:\"\n",
    "result1a = ex1('end','the')\n",
    "print \"Probability of \\'end\\' given \\'the\\': \" + str(result1a)\n",
    "result1b = ex1('the','end')\n",
    "print \"Probability of \\'the\\' given \\'end\\': \" + str(result1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIDSTONE, alpha=0.01:\n",
      "Probability of 'end' given 'the': 0.00575913341952\n",
      "Probability of 'the' given 'end': 8.47242226553e-05\n",
      "LIDSTONE, alpha=0:\n",
      "Probability of 'end' given 'the': 0.00584652862363\n",
      "Probability of 'the' given 'end': 0.0\n",
      "LIDSTONE, alpha=1:\n",
      "Probability of 'end' given 'the': 0.00237913970308\n",
      "Probability of 'the' given 'end': 0.00015487068298\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 2 ####################\n",
    "# Solution for exercise 2\n",
    "# Input: word (string), context (string), alpha (float)\n",
    "# Output: p (float)\n",
    "# Compute the Lidstone smoothed probability for word given the single word context\n",
    "# Alpha is the smoothing parameter, normally between 0 and 1.\n",
    "def ex2(word,context,alpha):\n",
    "    p =0.0\n",
    "\n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "    austen_bigrams = zip(austen_words[:-1], austen_words[1:])  # list of bigrams as tuples\n",
    "\n",
    "    # compute the vocabulary size\n",
    "    V = len(set(austen_words))\n",
    "\n",
    "    # Compute probability of word given context\n",
    "    num = np.sum([tup == (context, word) for tup in austen_bigrams]) + alpha\n",
    "    denom = np.sum([tup[0] == context for tup in austen_bigrams]) + alpha*V\n",
    "    p = num / float(denom)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 2\n",
    "print \"LIDSTONE, alpha=0.01:\"\n",
    "result2a = ex2('end','the',.01)\n",
    "print \"Probability of \\'end\\' given \\'the\\': \" + str(result2a)\n",
    "result2b = ex2('the','end',.01)\n",
    "print \"Probability of \\'the\\' given \\'end\\': \" + str(result2b)\n",
    "print \"LIDSTONE, alpha=0:\"\n",
    "result2c = ex2('end','the',0)\n",
    "print \"Probability of \\'end\\' given \\'the\\': \" + str(result2c)\n",
    "result2d = ex2('the','end',0)\n",
    "print \"Probability of \\'the\\' given \\'end\\': \" + str(result2d)\n",
    "print \"LIDSTONE, alpha=1:\"\n",
    "result2e = ex2('end','the',1)\n",
    "print \"Probability of \\'end\\' given \\'the\\': \" + str(result2e)\n",
    "result2f = ex2('the','end',1)\n",
    "print \"Probability of \\'the\\' given \\'end\\': \" + str(result2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKOFF WITH LAPLACE\n",
      "Probability of 'end' given 'the': 0.0023789133124\n",
      "backing off for ('end', 'the')\n",
      "Probability of 'the' given 'end': 0.0339454865864\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 3 ####################\n",
    "# Solution for exercise 3\n",
    "# Input: word (string), context (string)\n",
    "# Output: p (float)\n",
    "def ex3(word,context):\n",
    "    p =0.0\n",
    "\n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "\n",
    "    # Train a bigram language model using a LAPLACE estimator AND BACKOFF\n",
    "    lm = NgramModel(2,austen_words,estimator=lambda f,b: LaplaceProbDist(f,b+1))\n",
    "    # b+1 is necessary to provide, as it were, a bin for unseen contexts,\n",
    "    # so there is some probability left for the backoff probability, i.e. so\n",
    "    # that alpha is > 0.\n",
    "\n",
    "    # Compute probability of word given context.\n",
    "    # This method takes two arguments: the word and a *list* of words\n",
    "    # specifying its context.\n",
    "    # To see messages about backoff in action, use the named argument\n",
    "    # verbose = True.\n",
    "    p = lm.prob(word, [context], verbose=True)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 3\n",
    "print \"BACKOFF WITH LAPLACE\"\n",
    "result3a = ex3('end','the')\n",
    "print \"Probability of \\'end\\' given \\'the\\': \" + str(result3a)\n",
    "result3b = ex3('the','end')\n",
    "print \"Probability of \\'the\\' given \\'end\\': \" + str(result3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cross-entropy for austen-emma.txt: 2811146.28844\n",
      "Total cross-entropy for chesterton-ball.txt: 1678081.01156\n",
      "Per-word cross-entropy for austen-emma.txt: 23.7557258205\n",
      "Per-word cross-entropy for chesterton-ball.txt: -0.0\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 4 ####################\n",
    "\n",
    "# Solution for exercise 4 - entropy calculation\n",
    "# Input: lm (NgramModel language model), doc_name (string)\n",
    "# Output: e (float)\n",
    "def ex4_tot_entropy(lm,doc_name):\n",
    "    e = 0.0\n",
    "\n",
    "    # Construct a list of lowercase words from the document (test document)\n",
    "    doc_words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Compute the TOTAL cross entropy of the text in doc_name\n",
    "    e = lm.entropy(doc_words, pad_left=True, pad_right=True)\n",
    "\n",
    "    # Return the entropy\n",
    "    return e\n",
    "\n",
    "# Solution for exercise 4 - per-word entropy calculation\n",
    "# Input: lm (NgramModel language model), doc_name (string)\n",
    "# Output: e (float)\n",
    "def ex4_perword_entropy(lm,doc_name):\n",
    "    e = 0.0\n",
    "\n",
    "    # Construct a list of lowercase words from the document (test document)\n",
    "    doc_words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Compute the PER-WORD cross entropy of the text in doc_name\n",
    "    e = lm.entropy(w, pad_left=True, pad_right=True, perItem=True)\n",
    "\n",
    "    # Return the entropy\n",
    "    return e\n",
    "\n",
    "\n",
    "# Solution for exercise 4 - language model training\n",
    "# Input: doc_name (string)\n",
    "# Output: l (language model)\n",
    "def ex4_lm(doc_name):\n",
    "    l = None\n",
    "\n",
    "    # Construct a list of lowercase words from the document (training data for lm)\n",
    "    doc_words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Train a trigram language model using doc_words with backoff and a Lidstone probability distribution with +0.01 added to the sample count for each bin\n",
    "    l = NgramModel(3,doc_words,estimator=lambda f,b:nltk.LidstoneProbDist(f,0.01,f.B()+1))\n",
    "\n",
    "    # Return the language model\n",
    "    return l\n",
    "\n",
    "## Uncomment to test exercise 4\n",
    "lm4 = ex4_lm('austen-sense.txt')\n",
    "result4a = ex4_tot_entropy(lm4,'austen-emma.txt')\n",
    "print \"Total cross-entropy for austen-emma.txt: \" + str(result4a)\n",
    "result4b = ex4_tot_entropy(lm4,'chesterton-ball.txt')\n",
    "print \"Total cross-entropy for chesterton-ball.txt: \" + str(result4b)\n",
    "result4c = ex4_perword_entropy(lm4,'austen-emma.txt')\n",
    "print \"Per-word cross-entropy for austen-emma.txt: \" + str(result4c)\n",
    "result4d = ex4_perword_entropy(lm4,'chesterton-ball.txt')\n",
    "print \"Per-word cross-entropy for chesterton-ball.txt: \" + str(result4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
