{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FNLP - Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# coding: utf-8\n",
    "\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "# Import numpy as we will need it to calculate mean and standard deviation\n",
    "import numpy as np\n",
    "\n",
    "# Import the Presidential inaugural speeches, Brown and CONLL corpora\n",
    "# conll2007 is not installed by default\n",
    "nltk.data.path.append('/group/sgwater/data/nltk_data')\n",
    "from nltk.corpus import inaugural, brown, conll2007\n",
    "\n",
    "# directory with special twitter module\n",
    "sys.path.extend(['/group/ltg/projects/fnlp', '/group/ltg/projects/fnlp/packages_2.6'])\n",
    "\n",
    "# Import the Twitter corpus and LgramModel\n",
    "from twitter import xtwc, LgramModel\n",
    "\n",
    "# Stopword list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "twitter_file_ids = xtwc.fileids()[11:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### SECTION A: COMPARING CORPORA ####################\n",
    "\n",
    "##### Solution for question 1 #####\n",
    "\n",
    "def get_corpus_tokens(corpus, list_of_files):\n",
    "    '''Get the tokens from (part of) a corpus\n",
    "\n",
    "    :type corpus: nltk.corpus.CorpusReader\n",
    "    :param corpus: An NLTK corpus\n",
    "    :type list_of_files: list(file)\n",
    "    :param list_of_files: files to read from\n",
    "    :rtype: list(str)\n",
    "    :return: the tokenised contents of the files'''\n",
    "\n",
    "    # Construct \"corpus_tokens\" (a list of all tokens in the corpus)\n",
    "    corpus_tokens = [w.lower() for w in corpus.words(list_of_files)]\n",
    "\n",
    "    # Return the list of corpus tokens\n",
    "    return corpus_tokens\n",
    "\n",
    "def q1(corpus, list_of_files):\n",
    "    '''Compute the average word type length from (part of) a corpus\n",
    "\n",
    "    :type corpus: nltk.corpus.CorpusReader\n",
    "    :param corpus: An NLTK corpus\n",
    "    :type list_of_files: list(str)\n",
    "    :param list_of_files: names of files to read from\n",
    "    :rtype: float\n",
    "    :return: the average word type length over all the files'''\n",
    "\n",
    "    # Get a list of all tokens in the corpus\n",
    "    corpus_tokens = get_corpus_tokens(corpus, list_of_files)\n",
    "\n",
    "    # Construct a list that contains the lengths for each word\n",
    "    #  type in the document\n",
    "    type_lengths = [len(w) for w in set(corpus_tokens)]  # already lowercase\n",
    "\n",
    "    # Find the average word type length\n",
    "    avg_type_length = np.mean(type_lengths)\n",
    "\n",
    "    # Return the average word type length of the document\n",
    "    return avg_type_length\n",
    "\n",
    "##### Solution for question 2 #####\n",
    "\n",
    "def q2():\n",
    "    '''Question: Why might the average type length be greater for\n",
    "       twitter data?\n",
    "\n",
    "    :rtype: str\n",
    "    :return: your answer'''\n",
    "\n",
    "    return \"\"\"\n",
    "    The Twitter corpus contains words where a single letter is emphasized for emphasis (\\\"blahhhhhh...\\\" or \\\"jealoussssss...\\\"), URLs, repeated words (\\\"passeipasseipasseipassei...\\\"), and other \\\"words\\\" that are not used in normal speech. In contrast, the longest inaugural word is \\\"antiphilosophists\\\".\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'antiphilosophists', u'misrepresentation', u'contradistinction', u'misrepresentation', u'instrumentalities', u'instrumentalities', u'instrumentalities', u'instrumentalities', u'instrumentalities', u'instrumentalities']\n",
      "[u'blahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh', u'http://g1.globo.com/noticias/planetabizarro/0,,mul1466801-6091,00-para+salvar+namoro+chinesa+quer+operar+e+virar+sosia+de+jessica+alba.html', u'jealousssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss', u'http://www.formaturismo.com.br/portal/default.asp?acta=7&ano_selecionadovc=2010&destinoid=54&anovc=2010&galeriaid=442&bt_ok.x=14&bt_ok.y=19', u'\\u4f50\\u5929\\u3055\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042\\u3042', u'hahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahahaha', u'\\u767d\\u3072\\u3052\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048\\u3048', u'n\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3\\xe3o', u'passeipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipasseipa', u'sadiuhfiudashfuidahsfiouhdasiufhdisauhfiuahdsfiuhaiudshfiudahsiufhiaudshfiudahsiufhadsiuhfiuadhsfiuhasdiuhfiuadshfiuhadissdhfiuashfoishfishd']\n"
     ]
    }
   ],
   "source": [
    "# Get top x longest words\n",
    "def get_longest_x(corpus, list_of_files, x):\n",
    "    words = get_corpus_tokens(corpus, list_of_files)\n",
    "    words.sort(key = len)\n",
    "    return words[-x:]\n",
    "\n",
    "x = 10\n",
    "print get_longest_x(inaugural, inaugural.fileids(), x)\n",
    "print get_longest_x(xtwc, twitter_file_ids, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Question 1 ***\n",
      "Average token length for inaugural corpus: 7.77\n",
      "Average token length for twitter corpus: 11.25\n",
      "*** Question 2 ***\n",
      "\n",
      "    The Twitter corpus contains words where a single letter is emphasized for emphasis (\"blahhhhhh...\" or \"jealoussssss...\"), URLs, repeated words (\"passeipasseipasseipassei...\"), and other \"words\" that are not used in normal speech. In contrast, the longest inaugural word is \"antiphilosophists\".\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "### Question 1\n",
    "print \"*** Question 1 ***\"\n",
    "answer1a = q1(inaugural,inaugural.fileids())\n",
    "print \"Average token length for inaugural corpus: %.2f\"%answer1a\n",
    "answer1b = q1(xtwc,twitter_file_ids)\n",
    "print \"Average token length for twitter corpus: %.2f\"%answer1b\n",
    "### Question 2\n",
    "print \"*** Question 2 ***\"\n",
    "answer2 = q2()\n",
    "print answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### SECTION B: DATA IN THE REAL WORLD ####################\n",
    "\n",
    "##### Solution for question 3 #####\n",
    "\n",
    "def q3(corpus, list_of_files, x):\n",
    "    '''Tabulate and plot the top x most frequently used word types\n",
    "       and their counts from the specified files in the corpus\n",
    "\n",
    "    :type corpus: nltk.corpus.CorpusReader\n",
    "    :param corpus: An NLTK corpus\n",
    "    :type list_of_files: list(str)\n",
    "    :param list_of_files: names of files to read from\n",
    "    :rtype: list(tuple(string,int))\n",
    "    :return: top x word types and their counts from the files'''\n",
    "\n",
    "    # Get a list of all tokens in the corpus\n",
    "    corpus_tokens = get_corpus_tokens(corpus, list_of_files)\n",
    "\n",
    "    # Construct a frequency distribution over the lowercased tokens in the document\n",
    "    fd_doc_types = nltk.FreqDist(corpus_tokens)  # already lowercase\n",
    "\n",
    "    # Find the top x most frequently used types in the document\n",
    "    top_types = fd_doc_types.most_common(x)\n",
    "\n",
    "    # Produce a plot showing the top x types and their frequencies\n",
    "    fd_doc_types.plot(x)\n",
    "\n",
    "    return top_types\n",
    "\n",
    "##### Solution for question 4 #####\n",
    "\n",
    "def q4(corpus_tokens):\n",
    "    '''Clean a list of corpus tokens\n",
    "\n",
    "    :type corpus_tokens: list(str)\n",
    "    :param corpus_tokens: (lowercased) corpus tokens\n",
    "    :rtype: list(str)\n",
    "    :return: cleaned list of corpus tokens'''\n",
    "\n",
    "    stops = list(stopwords.words(\"english\"))\n",
    "\n",
    "    # If token is alpha-numeric and NOT in the list of stopwords,\n",
    "    #  add it to cleaned_tokens\n",
    "    cleaned_corpus_tokens = [w for w in corpus_tokens if w.isalnum() and w not in stops]\n",
    "\n",
    "    return cleaned_corpus_tokens\n",
    "\n",
    "##### Solution for question 5 #####\n",
    "\n",
    "def q5(cleaned_corpus_tokens, x):\n",
    "    '''Tabulate and plot the top x most frequently used word types\n",
    "       and their counts from the corpus tokens\n",
    "\n",
    "    :type corpus_tokens: list(str)\n",
    "    :param corpus_tokens: (cleaned) corpus tokens\n",
    "    :rtype: list(tuple(string,int))\n",
    "    :return: top x word types and their counts from the files'''\n",
    "\n",
    "    # Construct a frequency distribution over the lowercased tokens in the document\n",
    "    fd_doc_types = nltk.FreqDist(cleaned_corpus_tokens)  # already lowercase\n",
    "\n",
    "    # Find the top x most frequently used types in the document\n",
    "    top_types = fd_doc_types.most_common(x)\n",
    "\n",
    "    # Produce a plot showing the top x types and their frequencies\n",
    "    fd_doc_types.plot(x)\n",
    "\n",
    "    # Return the top x most frequently used types\n",
    "    return top_types\n",
    "\n",
    "##### Solution for question 6 #####\n",
    "\n",
    "def q6():\n",
    "    '''Problem: URLs in twitter data\n",
    "\n",
    "    :rtype: str\n",
    "    :return: your answer'''\n",
    "\n",
    "    return \"\"\"\n",
    "    The biggest problem I found was the existence of very common words in languages that are not English. For example, the second and third most common words are 'de' and 'que,' which are Spanish for 'of' and 'what' (respectively). I assume that if we included Spanish stopwords or separated the tweets by language, then this problem would be alleviated.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Question 3 ***\n",
      "Most common 50 types for the inaugural corpus:\n",
      "[(u'the', 9906), (u'of', 6986), (u',', 6840), (u'and', 5139), (u'.', 4676), (u'to', 4432), (u'in', 2749), (u'a', 2193), (u'our', 2058), (u'that', 1726), (u'we', 1625), (u'be', 1460), (u'is', 1416), (u'it', 1367), (u'for', 1154), (u'by', 1066), (u'which', 1002), (u'have', 997), (u'with', 937), (u'as', 931), (u'not', 924), (u'will', 851), (u'i', 832), (u'this', 812), (u'all', 794), (u'are', 779), (u'their', 738), (u'but', 628), (u'has', 612), (u'government', 593), (u'its', 565), (u'people', 563), (u'from', 551), (u';', 544), (u'or', 542), (u'on', 520), (u'my', 491), (u'been', 482), (u'can', 465), (u'us', 455), (u'no', 453), (u'they', 440), (u'so', 383), (u'an', 380), (u'upon', 369), (u'--', 363), (u'who', 361), (u'must', 346), (u'at', 341), (u'may', 334)]\n",
      "Most common 50 types for the twitter corpus:\n",
      "[(u'.', 108250), (u':', 80794), (u',', 79419), (u'i', 57861), (u'!', 52645), (u\"'\", 49816), (u'the', 48974), (u'a', 48204), (u'rt', 43383), (u'to', 42070), (u'...', 37758), (u'-', 36160), (u'?', 32058), (u'you', 25994), (u'\\u3002', 23544), (u'and', 23124), (u'in', 21850), (u'it', 21699), (u'is', 20680), (u'of', 19861), (u'\"', 19159), (u's', 18953), (u'for', 18546), (u'de', 18386), (u'my', 18369), (u'on', 18345), (u'me', 17960), (u'..', 17075), (u'\\u3001', 16308), (u'(', 14486), (u'that', 14004), (u'que', 13207), (u'o', 12600), (u'&', 12551), (u'no', 12264), (u't', 11957), (u')', 11797), (u'do', 11235), (u'u', 10637), (u'at', 10407), (u';', 10062), (u'lol', 9735), (u'e', 9652), (u'/', 9490), (u'this', 9447), (u'just', 9307), (u'be', 9196), (u'with', 9171), (u'd', 9140), (u'so', 9077)]\n",
      "*** Question 4 ***\n",
      "Inaugural Speeches:\n",
      "Number of tokens in original corpus: 145735\n",
      "Number of tokens in cleaned corpus: 62726\n",
      "First 100 tokens in cleaned corpus:\n",
      "[u'fellow', u'citizens', u'senate', u'house', u'representatives', u'among', u'vicissitudes', u'incident', u'life', u'event', u'could', u'filled', u'greater', u'anxieties', u'notification', u'transmitted', u'order', u'received', u'14th', u'day', u'present', u'month', u'one', u'hand', u'summoned', u'country', u'whose', u'voice', u'never', u'hear', u'veneration', u'love', u'retreat', u'chosen', u'fondest', u'predilection', u'flattering', u'hopes', u'immutable', u'decision', u'asylum', u'declining', u'years', u'retreat', u'rendered', u'every', u'day', u'necessary', u'well', u'dear', u'addition', u'habit', u'inclination', u'frequent', u'interruptions', u'health', u'gradual', u'waste', u'committed', u'time', u'hand', u'magnitude', u'difficulty', u'trust', u'voice', u'country', u'called', u'sufficient', u'awaken', u'wisest', u'experienced', u'citizens', u'distrustful', u'scrutiny', u'qualifications', u'could', u'overwhelm', u'despondence', u'one', u'inheriting', u'inferior', u'endowments', u'nature', u'unpracticed', u'duties', u'civil', u'administration', u'ought', u'peculiarly', u'conscious', u'deficiencies', u'conflict', u'emotions', u'dare', u'aver', u'faithful', u'study', u'collect', u'duty', u'appreciation']\n",
      "-----\n",
      "Twitter:\n",
      "Number of tokens in original corpus: 4091118\n",
      "Number of tokens in cleaned corpus: 2229862\n",
      "First 100 tokens in cleaned corpus:\n",
      "[u'd', u'hmm', u'like', u'see', u'would', u'2', u'cause', u'mit', u'video', u'capture', u'youtube', u'funktion', u'complete', u'dinner', u'filet', u'guess', u'haha', u'made', u'making', u'parents', u'roast', u'smell', u'throw', u'transformation', u'want', u'6', u'de', u'han', u'ja', u'ja', u'jajajaja', u'mandado', u'mi', u'nada', u'ninguna', u'sufreee', u'tarea', u'detenidos', u'omar', u'esparragoza', u'v\\xedctor', u'figuera', u'garc\\xeda', u'richard', u'le\\xf3n', u'lu\\xeds', u'mart\\xednez', u'audervis', u'reyes', u'h\\xe9ctor', u'rt', u'rt', u'fude', u'odeio', u'se', u'te', u'vai', u'voce', u'\\u5b50\\u4f9b\\u306e\\u9803\\u306b\\u5b50\\u4f9b\\u306e\\u9803\\u306b\\u5922\\u4e2d\\u3067\\u63a2\\u3057\\u3066\\u305f\\u3082\\u306e\\u304c\\u307b\\u3089\\u4eca\\u76ee\\u306e\\u524d\\u3067\\u624b\\u3092\\u5e83\\u3052\\u3066\\u3044\\u308b', u'blnded', u'bn', u'bt', u'dnt', u'ex', u'got', u'jus', u'luv', u'nw', u'omg', u'warnd', u'waz', u'yr', u'piss', u'll', u'agora', u'at\\xe9', u'os', u'produtos', u'rt', u'vejam', u'vendidos', u'dsa', u'1980', u'ircd', u'hybrid', u'ircd', u'ratbox', u'\\u79c1\\u306e\\u89aa\\u3057\\u3044\\u4eba\\u3005\\u304c\\u5e78\\u305b\\u3067\\u3042\\u308a\\u307e\\u3059\\u3088\\u3046\\u306b', u'd\\xe1', u'lhe', u'd\\xe1', u'lhe', u'alpha', u'mann', u'anziehst', u'du', u'du', u'frauen', u'geheimnisse', u'magnetisch']\n",
      "*** Question 5 ***\n",
      "Most common 50 types for the cleaned inaugural corpus:\n",
      "[(u'government', 593), (u'people', 563), (u'us', 455), (u'upon', 369), (u'must', 346), (u'may', 334), (u'great', 331), (u'world', 329), (u'states', 329), (u'shall', 314), (u'country', 302), (u'nation', 302), (u'every', 285), (u'peace', 252), (u'one', 243), (u'new', 240), (u'citizens', 237), (u'power', 230), (u'public', 225), (u'would', 209), (u'time', 206), (u'constitution', 205), (u'united', 197), (u'nations', 194), (u'america', 192), (u'union', 188), (u'freedom', 183), (u'free', 179), (u'war', 175), (u'national', 154), (u'made', 151), (u'let', 149), (u'fellow', 148), (u'american', 147), (u'men', 144), (u'good', 143), (u'spirit', 138), (u'without', 138), (u'well', 137), (u'rights', 136), (u'years', 134), (u'law', 134), (u'justice', 134), (u'life', 133), (u'make', 133), (u'laws', 130), (u'congress', 129), (u'never', 125), (u'best', 120), (u'liberty', 116)]\n",
      "Most common 50 types for the cleaned twitter corpus:\n",
      "[(u'rt', 43383), (u'de', 18386), (u'que', 13207), (u'o', 12600), (u'u', 10637), (u'lol', 9735), (u'e', 9652), (u'd', 9140), (u'm', 8717), (u'like', 7219), (u'get', 6632), (u'eu', 6323), (u'com', 6264), (u'2', 6160), (u'lt', 5776), (u'\\xe9', 5718), (u'love', 5586), (u'good', 5476), (u'new', 5464), (u'la', 5426), (u'q', 5083), (u'go', 4794), (u'da', 4713), (u'one', 4700), (u'gt', 4645), (u'3', 4518), (u'haha', 4469), (u'en', 4432), (u'know', 4344), (u'n\\xe3o', 4320), (u'se', 4280), (u'y', 4273), (u'1', 4100), (u'time', 4075), (u'ya', 4037), (u'got', 4016), (u'day', 4002), (u'um', 3942), (u'im', 3742), (u'el', 3676), (u'p', 3610), (u'twitter', 3465), (u'pra', 3454), (u'em', 3423), (u'today', 3352), (u'ipad', 3344), (u'na', 3343), (u'por', 3304), (u'think', 3204), (u'see', 3185)]\n",
      "*** Question 6 ***\n",
      "\n",
      "    The biggest problem I found was the existence of very common words in languages that are not English. For example, the second and third most common words are 'de' and 'que,' which are Spanish for 'of' and 'what' (respectively). I assume that if we included Spanish stopwords or separated the tweets by language, then this problem would be alleviated.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "### Question 3\n",
    "print \"*** Question 3 ***\"\n",
    "print \"Most common 50 types for the inaugural corpus:\"\n",
    "answer3a = q3(inaugural,inaugural.fileids(),50)\n",
    "print answer3a\n",
    "print \"Most common 50 types for the twitter corpus:\"\n",
    "answer3b = q3(xtwc,twitter_file_ids,50)\n",
    "print answer3b\n",
    "### Question 4\n",
    "print \"*** Question 4 ***\"\n",
    "corpus_tokens = get_corpus_tokens(inaugural,inaugural.fileids())\n",
    "answer4a = q4(corpus_tokens)\n",
    "print \"Inaugural Speeches:\"\n",
    "print \"Number of tokens in original corpus: %s\"%len(corpus_tokens)\n",
    "print \"Number of tokens in cleaned corpus: %s\"%len(answer4a)\n",
    "print \"First 100 tokens in cleaned corpus:\"\n",
    "print answer4a[:100]\n",
    "print \"-----\"\n",
    "corpus_tokens = get_corpus_tokens(xtwc,twitter_file_ids)\n",
    "answer4b = q4(corpus_tokens)\n",
    "print \"Twitter:\"\n",
    "print \"Number of tokens in original corpus: %s\"%len(corpus_tokens)\n",
    "print \"Number of tokens in cleaned corpus: %s\"%len(answer4b)\n",
    "print \"First 100 tokens in cleaned corpus:\"\n",
    "print answer4b[:100]\n",
    "### Question 5\n",
    "print \"*** Question 5 ***\"\n",
    "print \"Most common 50 types for the cleaned inaugural corpus:\"\n",
    "answer5a = q5(answer4a, 50)\n",
    "print answer5a\n",
    "print \"Most common 50 types for the cleaned twitter corpus:\"\n",
    "answer5b = q5(answer4b, 50)\n",
    "print answer5b\n",
    "### Question 6\n",
    "print \"*** Question 6 ***\"\n",
    "answer6 = q6()\n",
    "print answer6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LgramModel in module twitter:\n",
      "\n",
      "class LgramModel(nltkx.model.ngram.NgramModel)\n",
      " |  Method resolution order:\n",
      " |      LgramModel\n",
      " |      nltkx.model.ngram.NgramModel\n",
      " |      nltkx.model.api.ModelI\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n, train, pad_left=False, pad_right=False, estimator=None, *estimator_args, **estimator_kwargs)\n",
      " |      NgramModel (q.v.) slightly tweaked to produce char-grams,\n",
      " |      not word-grams, with a WittenBell default estimator\n",
      " |      \n",
      " |      :param train: List of strings, which will be converted to list of lists of characters, but more efficiently\n",
      " |      :type train: iter(str)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltkx.model.ngram.NgramModel:\n",
      " |  \n",
      " |  __contains__(self, item)\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      x.__str__() <==> str(x)\n",
      " |  \n",
      " |  __unicode__ = __str__(...)\n",
      " |      x.__str__() <==> str(x)\n",
      " |  \n",
      " |  dump(self, file, logBase=None, precision=7)\n",
      " |      Dump this model in SRILM/ARPA/Doug Paul format\n",
      " |      \n",
      " |      Use logBase=10 and the default precision to get something comparable\n",
      " |      to SRILM ngram-model -lm output\n",
      " |      @param file to dump to\n",
      " |      @type file file\n",
      " |      @param logBase If not None, output logBases to the specified base\n",
      " |      @type logBase int|None\n",
      " |  \n",
      " |  entropy(self, text, pad_left=False, pad_right=False, verbose=False, perItem=False)\n",
      " |      Calculate the approximate cross-entropy of the n-gram model for a\n",
      " |      given evaluation text.\n",
      " |      This is the average log probability of each item in the text.\n",
      " |      \n",
      " |      :param text: items to use for evaluation\n",
      " |      :type text: iterable(str)\n",
      " |      :param pad_left: whether to pad the left of each text with an (n-1)-gram of <s> markers\n",
      " |      :type pad_left: bool\n",
      " |      :param pad_right: whether to pad the right of each sentence with an </s> marker\n",
      " |      :type pad_right: bool\n",
      " |      :param perItem: normalise for length if True\n",
      " |      :type perItem: bool\n",
      " |  \n",
      " |  generate(self, num_words, context=())\n",
      " |      Generate random text based on the language model.\n",
      " |      \n",
      " |      :param num_words: number of words to generate\n",
      " |      :type num_words: int\n",
      " |      :param context: initial words in generated string\n",
      " |      :type context: list(str)\n",
      " |  \n",
      " |  logprob(self, word, context, verbose=False)\n",
      " |      Evaluate the (negative) log probability of this word in this context.\n",
      " |      \n",
      " |      :param word: the word to get the probability of\n",
      " |      :type word: str\n",
      " |      :param context: the context the word is in\n",
      " |      :type context: list(str)\n",
      " |  \n",
      " |  perplexity(self, text, pad_left=False, pad_right=False, verbose=False)\n",
      " |      Calculates the perplexity of the given text.\n",
      " |      This is simply 2 ** cross-entropy for the text.\n",
      " |      \n",
      " |      :param text: words to calculate perplexity of\n",
      " |      :type text: list(str)\n",
      " |      :param pad_left: whether to pad the left of each sentence with an (n-1)-gram of empty strings\n",
      " |      :type pad_left: bool\n",
      " |      :param pad_right: whether to pad the right of each sentence with an (n-1)-gram of empty strings\n",
      " |      :type pad_right: bool\n",
      " |  \n",
      " |  prob(self, word, context, verbose=False)\n",
      " |      Evaluate the probability of this word in this context using Katz Backoff.\n",
      " |      \n",
      " |      :param word: the word to get the probability of\n",
      " |      :type word: str\n",
      " |      :param context: the context the word is in\n",
      " |      :type context: list(str)\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltkx.model.ngram.NgramModel:\n",
      " |  \n",
      " |  backoff\n",
      " |  \n",
      " |  model\n",
      " |  \n",
      " |  ngrams\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltkx.model.api.ModelI:\n",
      " |  \n",
      " |  choose_random_word(self, context)\n",
      " |      Randomly select a word that is likely to appear in this context.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltkx.model.api.ModelI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LgramModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### SECTION C: LANGUAGE IDENTIFICATION ####################\n",
    "\n",
    "##### Solution for question 7 #####\n",
    "\n",
    "def q7(corpus):\n",
    "    '''Build a bigram letter language model using LgramModel\n",
    "       based on the all-alpha subset the entire corpus\n",
    "\n",
    "    :type corpus: nltk.corpus.CorpusReader\n",
    "    :param corpus: An NLTK corpus\n",
    "    :rtype: LgramModel\n",
    "    :return: A padded letter bigram model based on nltk.model.NgramModel'''\n",
    "\n",
    "    corpus_tokens = [w.lower() for w in corpus.words() if w.isalpha()]\n",
    "\n",
    "    bigram_model = LgramModel(2, corpus_tokens, pad_left=True, pad_right=True)\n",
    "\n",
    "    # Return the letter bigram LM\n",
    "    return bigram_model\n",
    "\n",
    "##### Solution for question 8 #####\n",
    "\n",
    "def q8(file_name,bigram_model):\n",
    "    '''Using a character bigram model, compute sentence entropies\n",
    "       for a subset of the tweet corpus, removing all non-alpha tokens and\n",
    "       tweets with less than 5 all-alpha tokens\n",
    "\n",
    "    :type file_name: str\n",
    "    :param file_name: twitter file to process\n",
    "    :rtype: list(tuple(float,list(str)))\n",
    "    :return: ordered list of average entropies and tweets'''\n",
    "\n",
    "    list_of_tweets = xtwc.sents(file_name)\n",
    "\n",
    "    cleaned_list_of_tweets = []\n",
    "    for tweet in list_of_tweets:\n",
    "        cleaned_tweet = [w.lower() for w in tweet if w.isalpha()]\n",
    "        if len(cleaned_tweet) >= 5:\n",
    "            cleaned_list_of_tweets.append(cleaned_tweet)\n",
    "    \n",
    "    # For each tweet in the cleaned corpus, compute the average word\n",
    "    #  entropy, and store in a list of tuples of the form: (entropy,tweet)\n",
    "    list_of_tweets_and_entropies = []\n",
    "    for tweet in cleaned_list_of_tweets:\n",
    "        e = np.mean([bigram_model.entropy(w, pad_left=True, pad_right=True, perItem=True) for w in tweet])\n",
    "        list_of_tweets_and_entropies.append((e, tweet))\n",
    "            \n",
    "    \n",
    "    # Sort the list of (entropy,tweet) tuples by entropy\n",
    "    list_of_tweets_and_entropies = sorted(list_of_tweets_and_entropies, key=lambda tup: tup[0])\n",
    "\n",
    "    # Return the sorted list of tuples\n",
    "    return list_of_tweets_and_entropies\n",
    "\n",
    "##### Solution for question 9 #####\n",
    "\n",
    "def q9():\n",
    "    '''Question: What differentiates the beginning and end of the list\n",
    "       of tweets and their entropies?\n",
    "\n",
    "    :rtype: str\n",
    "    :return: your answer'''\n",
    "\n",
    "    return \"\"\"...\n",
    "    \"\"\"\n",
    "\n",
    "##### Solution for question 10 #####\n",
    "\n",
    "# Output:\n",
    "def q10(list_of_tweets_and_entropies):\n",
    "    '''Compute entropy mean, standard deviation and using them,\n",
    "       likely non-English tweets in the all-ascii subset of list of tweets\n",
    "       and their biletter entropies\n",
    "\n",
    "    :type list_of_tweets_and_entropies: list(tuple(float,list(str)))\n",
    "    :param list_of_tweets_and_entropies: tweets and their\n",
    "                                    internal average biletter entropy\n",
    "    :rtype: tuple(float, float, list(tuple(float,list(str)))\n",
    "    :return: mean, standard deviation, ascii tweets and entropies,\n",
    "             not-English tweets and entropies'''\n",
    "\n",
    "    # Find the \"ascii\" tweets - those in the lowest-entropy 90%\n",
    "    #  of list_of_tweets_and_entropies\n",
    "    threshold = int(len(list_of_tweets_and_entropies) * 0.9)\n",
    "    list_of_ascii_tweets_and_entropies = list_of_tweets_and_entropies[:threshold]\n",
    "\n",
    "    # Extract a list of just the entropy values\n",
    "    list_of_entropies = [tup[0] for tup in list_of_ascii_tweets_and_entropies]\n",
    "\n",
    "    # Compute the mean of entropy values for \"ascii\" tweets\n",
    "    mean = np.mean(list_of_entropies)\n",
    "\n",
    "    # Compute their standard deviation\n",
    "    standard_deviation = np.std(list_of_entropies)\n",
    "\n",
    "    # Get a list of \"probably not English\" tweets, that is, \"ascii\"\n",
    "    # tweets with an entropy greater than (mean + (0.674 * std_dev))\n",
    "    threshold = mean + (0.674 * standard_deviation)\n",
    "    list_of_not_English_tweets_and_entropies = [tup for tup in list_of_ascii_tweets_and_entropies if tup[0] > threshold]\n",
    "    \n",
    "    # sort...\n",
    "    list_of_not_English_tweets_and_entropies = sorted(list_of_not_English_tweets_and_entropies, key=lambda tup: tup[0])\n",
    "\n",
    "    # Return the mean and standard_deviation values and the two lists\n",
    "    return (mean, standard_deviation,\n",
    "            list_of_ascii_tweets_and_entropies,\n",
    "            list_of_not_English_tweets_and_entropies)\n",
    "\n",
    "##### Solution for question 11 #####\n",
    "\n",
    "def q11(list_of_files, list_of_not_English_tweets_and_entropies):\n",
    "    '''Build a padded spanish bigram letter bigram model and use it\n",
    "       to re-sort the probably-not-English data\n",
    "\n",
    "    :type list_of_files: list(str)\n",
    "    :param list_of_files: spanish corpus files\n",
    "    :type list_of_tweets_and_entropies: list(tuple(float,list(str)))\n",
    "    :param list_of_tweets_and_entropies: tweets and their\n",
    "                                    internal average biletter entropy\n",
    "    :rtype: list(tuple(float,list(str)))\n",
    "    :return: probably-not-English tweets and _spanish_ entropies'''\n",
    "\n",
    "    # Build a bigram letter language model using \"LgramModel\"\n",
    "    corpus_tokens = [w.lower() for w in conll2007.words(list_of_files) if w.isalpha()]\n",
    "    bigram_model = LgramModel(2, corpus_tokens, pad_left=True, pad_right=True)\n",
    "\n",
    "    # Compute the entropy of each of the tweets in list (list_of_not_English_tweets_and_entropies) using the new bigram letter language model\n",
    "    # list_of_not_English_tweets_and_entropies = [(bigram_model.entropy(tup[1], pad_left=True, pad_right=True, perItem=True), tup[1]) for tup in list_of_not_English_tweets_and_entropies]\n",
    "    tweets = [tup[1] for tup in list_of_not_English_tweets_and_entropies]\n",
    "    list_of_not_English_tweets_and_entropies = []\n",
    "    for tweet in tweets:\n",
    "        e = np.mean([bigram_model.entropy(w, pad_left=True, pad_right=True, perItem=True) for w in tweet])\n",
    "        list_of_not_English_tweets_and_entropies.append((e, tweet))\n",
    "\n",
    "    # Sort the new list of (entropy,tweet) tuples\n",
    "    list_of_not_English_tweets_and_entropies = sorted(list_of_not_English_tweets_and_entropies, key=lambda tup: tup[0])\n",
    "\n",
    "    # Return the list of tweets with _new_ entropies, re-sorted\n",
    "    return list_of_not_English_tweets_and_entropies\n",
    "\n",
    "\n",
    "##### Answers #####\n",
    "\n",
    "def ppEandT(eAndTs):\n",
    "    '''Pretty print a list of entropy+tweet pairs\n",
    "\n",
    "    :type eAndTs: list(tuple(float,list(str)))\n",
    "    :param eAndTs: entropies and tweets\n",
    "    :return: None'''\n",
    "\n",
    "    for entropy,tweet in eAndTs:\n",
    "        print (u\"%.3f {%s}\"%(entropy,\", \".join(tweet))).encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Question 7: building brown bigram letter model ***\n",
      "*** Question 8 ***\n",
      "Best 10 entropies:\n",
      "2.492 {and, here, is, proof, the}\n",
      "2.539 {and, bailed, he, here, is, man, on, that, the}\n",
      "2.558 {is, the, this, weather, worst}\n",
      "2.569 {s, s, s, s, s, s, s, s, s, s}\n",
      "2.570 {be, bus, here, the, to, want}\n",
      "2.577 {hell, that, the, was, wat}\n",
      "2.588 {creation, is, of, on, story, the, the}\n",
      "2.589 {fro, one, the, the, with}\n",
      "2.595 {is, money, motive, the, the}\n",
      "2.618 {at, bucks, end, lead, of, the, the, the}\n",
      "Worst 10 entropies:\n",
      "17.524 {作品によっては怪人でありながらヒーロー, あるいはその逆, というシチュエーションも多々ありますが, そうした事がやれるのもやはり怪人とヒーローと言うカテゴリが完成しているからだと思うんですよね, あれだけのバリエーションがありながららしさを失わないデザインにはまさに感服です}\n",
      "17.525 {ロンブーの淳さんはスピリチュアルスポット, セドナーで瞑想を実践してた, これらは偶然ではなく必然的に起こっている, 自然は全て絶好のタイミングで教えてくれている, そして今が今年最大の大改革時期だ}\n",
      "17.526 {実物経済と金融との乖離を際限なく広げる, レバレッジが金融で儲けるコツだと, まるで正義のように叫ぶ連中が多いけど, これほど不健全な金融常識はないと思う, 連中は不健全と知りながら, 他の奴がやるから出し抜かれる前に出し抜くのが道理と言わんばかりに群がる}\n",
      "17.528 {一応ワンセット揃えてみたんだけど, イマイチ効果を感じないのよね, それよりはオーラソーマとか, 肉体に直接働きかけるタイプのアプローチの方が効き目を感じ取りやすい, 波動系ならバッチよりはホメオパシーの方がわかりやすい}\n",
      "17.533 {慶喜ほどの人でさえこうなんだから, 並の人間だったらなおさら参謀無しじゃ何も出来ない, 一般に吹聴されてる慶喜のネガティブ論は, こうした敵対勢力による相次ぐテロに対して終始無関心で, 慶喜個人だけに批判を向けがち}\n",
      "17.541 {昨日のセミナーではお目にかかれて光栄でした, 楽しく充実した時間をありがとうございました, 親しみのもてる分かりやすい講演に勇気を頂きました, 素晴らしいお仕事とともに益々のご活躍願っております, 今後ともよろしくお願いします}\n",
      "17.541 {自民党が小沢やめろというなら, 当然町村やめろというブーメランがかえってくるわけです, おふたりとも選挙で選ばれた正当な国民の代表ですから, できればどちらにもやめてほしくありません, そろそろこんな不毛なことはやめにしてほしい}\n",
      "17.543 {知識欲というのは不随意筋でできている, どうせ人間には永久に解明できないんだから, 宇宙はある時点で生まれたのか, それとも永遠の過去から存在しているのかなんてことを追究するなと言ってもムダだ, 心臓に止まれと命令しても止まらないのと同じことだ}\n",
      "17.548 {と言いつつもやっぱり笑えない時はあるよなあ, 笑っても自分の笑顔が汚らわしく思えてすぐ止めちゃうの, 自分が息してるだけで悲しくてぼろぼろ泣いてる時期もあった, 今の自分に必要な経験だったとは思うけど, 出来ればあんな感情は二度とごめんだ}\n",
      "17.553 {中身の羽毛は精製過程で殺菌処理しているから, 羽毛布団からダニが湧くことはない, あと羽毛布団の生地は糸の打ち込み本数が多く, 羽毛の吹き出しを防ぐ目つぶし加工をしているからダニは羽毛ふとんの生地を通過できない, ただダニが布団に付着することはあるから手入れは必要}\n",
      "*** Question 9 ***\n",
      "...\n",
      "    \n",
      "*** Question 10 ***\n",
      "Mean: 3.84189241296\n",
      "Standard Deviation: 0.476377618438\n",
      "==================\n",
      "'Ascii' tweets: Best 10 entropies:\n",
      "2.492 {and, here, is, proof, the}\n",
      "2.539 {and, bailed, he, here, is, man, on, that, the}\n",
      "2.558 {is, the, this, weather, worst}\n",
      "2.569 {s, s, s, s, s, s, s, s, s, s}\n",
      "2.570 {be, bus, here, the, to, want}\n",
      "2.577 {hell, that, the, was, wat}\n",
      "2.588 {creation, is, of, on, story, the, the}\n",
      "2.589 {fro, one, the, the, with}\n",
      "2.595 {is, money, motive, the, the}\n",
      "2.618 {at, bucks, end, lead, of, the, the, the}\n",
      "==================\n",
      "'Ascii' tweets: Worst 10 entropies:\n",
      "5.158 {esse, menino, naao, saidinho, uaehuae}\n",
      "5.158 {bij, grote, grutjes, internet, komen, ligt, mijn, mogelijk, plat, snel, terug, u, wij, zo}\n",
      "5.158 {akuur, dari, ga, kami, lol, mmng, rumah, tangga, udah, waaah}\n",
      "5.159 {a, aêeee, cuscuz, dar, do, gente, kkkkkkkkkkkkkk, morro, nath, no, só, vai}\n",
      "5.159 {animada, até, clube, cóe, continuar, curte, daqui, de, eu, glória, hahaha, hj, lá, mais, quem, quinta, se, tarde, ver, vou}\n",
      "5.159 {bonding, rt, brokoloiiiii, hahaahhahha, ckckckck, haha, rt, suru}\n",
      "5.159 {atropelada, comemoração, e, é, em, eua, ganha, humano, loteria, mil, morre, mulher, na, não, não, nos, quem, r, rir}\n",
      "5.159 {ap, beneran, bsk, bunu, bunuh, deh, diri, dy, free, gw, gw, gw, hahaha, k, kga, kl, lu, nyampe, orgny, pkkny, plihanny, sih, ya}\n",
      "5.159 {ad, bisa, dr, gk, lebih, minggu, ya, yg}\n",
      "5.159 {aku, aku, apaan, bngt, demo, demo, ema, gatau, gt, haha, hari, iya, iya, jg, kak, kata, mksdny, parah, rt, rt, sby, temen, ya}\n",
      "==================\n",
      "Probably not English tweets: Best 10 entropies:\n",
      "4.163 {caulfield, cinema, com, e, me, mimimi, não, odeia, venham}\n",
      "4.163 {apresenta, carro, com, dos, e, ferrari, novo, retorno, sonha, títulos}\n",
      "4.163 {encomoda, foda, hoje, maiis, porcoo, que, s, seer, sobrinha, soltoo, vai}\n",
      "4.163 {aprile, claudio, fiona, leheup, lim, rebecca, sismondo, sweete}\n",
      "4.163 {css, een, eerst, firefox, gebruiken, hack, het, in, moeten, only, tja, voor}\n",
      "4.163 {d, dia, e, hoje, nublaado, tenebroso}\n",
      "4.163 {a, t, but, confused, don, t, for, get, i, i, jk, jk, lol, not, prettyboy, rt, think, worry, your}\n",
      "4.163 {aparelho, bola, gato, colocar, fixo, meu, nas, nem, p, pensar, pra, próximas, rt, saindo, semanas}\n",
      "4.163 {bbm, same, still, the, ya, yoo}\n",
      "4.163 {ah, aqui, aya, caido, como, en, esta, nada, no, para, paterson, pero}\n",
      "==================\n",
      "Probably not English tweets: Worst 10 entropies:\n",
      "5.158 {esse, menino, naao, saidinho, uaehuae}\n",
      "5.158 {bij, grote, grutjes, internet, komen, ligt, mijn, mogelijk, plat, snel, terug, u, wij, zo}\n",
      "5.158 {akuur, dari, ga, kami, lol, mmng, rumah, tangga, udah, waaah}\n",
      "5.159 {a, aêeee, cuscuz, dar, do, gente, kkkkkkkkkkkkkk, morro, nath, no, só, vai}\n",
      "5.159 {animada, até, clube, cóe, continuar, curte, daqui, de, eu, glória, hahaha, hj, lá, mais, quem, quinta, se, tarde, ver, vou}\n",
      "5.159 {bonding, rt, brokoloiiiii, hahaahhahha, ckckckck, haha, rt, suru}\n",
      "5.159 {atropelada, comemoração, e, é, em, eua, ganha, humano, loteria, mil, morre, mulher, na, não, não, nos, quem, r, rir}\n",
      "5.159 {ap, beneran, bsk, bunu, bunuh, deh, diri, dy, free, gw, gw, gw, hahaha, k, kga, kl, lu, nyampe, orgny, pkkny, plihanny, sih, ya}\n",
      "5.159 {ad, bisa, dr, gk, lebih, minggu, ya, yg}\n",
      "5.159 {aku, aku, apaan, bngt, demo, demo, ema, gatau, gt, haha, hari, iya, iya, jg, kak, kata, mksdny, parah, rt, rt, sby, temen, ya}\n",
      "*** Question 11 ***\n",
      "Best 10 entropies:\n",
      "2.741 {cancion, es, esa, lo, más}\n",
      "2.779 {del, día, es, esta, foto, la, via}\n",
      "2.804 {alegrarnos, de, día, el, linda, manera, que}\n",
      "2.808 {allá, de, es, gatos, lo, los, más, normales, que, que, ve, ven}\n",
      "2.810 {aburriiiida, contabilidad, cosa, de, es, la, la, la, más, s, vida}\n",
      "2.882 {es, los, prefiero, que, subtítulos}\n",
      "2.904 {catalán, de, eres, esto, opinas, que, vía}\n",
      "2.919 {a, amaneció, carriles, da, de, de, de, descuido, día, el, el, era, está, hecho, lado, madrazo, te, un, un, un, viaducto, y, y}\n",
      "2.920 {el, es, está, la, malo, méxico, no, para, planeado, que}\n",
      "2.934 {brillar, de, día, donde, escondía, la, luna, se, ver}\n",
      "Worst 10 entropies:\n",
      "6.923 {two, weeks, decimated, tsv, two, weeks, tsv}\n",
      "6.923 {two, weeks, decimated, tsv, two, weeks, tsv}\n",
      "6.927 {follow, gt, gt, gt, gt, gt, gt, gt, gt, rt}\n",
      "6.944 {follow, gt, gt, gt, gt, gt, gt, gt, now}\n",
      "7.012 {bettahh, okay, ooh, ooh, they}\n",
      "7.050 {follow, gt, gt, gt, gt, gt, gt, gt, gt}\n",
      "7.087 {eww, omg, really, sickk, thts}\n",
      "7.089 {follow, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt}\n",
      "7.089 {follow, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt}\n",
      "7.197 {elysium, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, gt, playing}\n"
     ]
    }
   ],
   "source": [
    "### Question 7\n",
    "print \"*** Question 7: building brown bigram letter model ***\"\n",
    "brown_bigram_model = q7(brown) ########################################\n",
    "### Question 8\n",
    "print \"*** Question 8 ***\"\n",
    "answer8 = q8(\"20100128.txt\",brown_bigram_model)\n",
    "print \"Best 10 entropies:\"\n",
    "ppEandT(answer8[:10])\n",
    "print \"Worst 10 entropies:\"\n",
    "ppEandT(answer8[-10:])\n",
    "### Question 9\n",
    "print \"*** Question 9 ***\"\n",
    "answer9 = q9()\n",
    "print answer9\n",
    "### Question 10\n",
    "print \"*** Question 10 ***\"\n",
    "answer10 = q10(answer8)\n",
    "print \"Mean: %s\"%answer10[0]\n",
    "print \"Standard Deviation: %s\"%answer10[1]\n",
    "print \"==================\"\n",
    "print \"'Ascii' tweets: Best 10 entropies:\"\n",
    "ppEandT(answer10[2][:10])\n",
    "print \"==================\"\n",
    "print \"'Ascii' tweets: Worst 10 entropies:\"\n",
    "ppEandT(answer10[2][-10:])\n",
    "print \"==================\"\n",
    "print \"Probably not English tweets: Best 10 entropies:\"\n",
    "ppEandT(answer10[3][:10])\n",
    "print \"==================\"\n",
    "print \"Probably not English tweets: Worst 10 entropies:\"\n",
    "ppEandT(answer10[3][-10:])\n",
    "### Question 11\n",
    "print \"*** Question 11 ***\"\n",
    "list_of_not_English_tweets_and_entropies = answer10[3]\n",
    "answer11 = q11([\"esp.test\",\"esp.train\"],list_of_not_English_tweets_and_entropies)\n",
    "print \"Best 10 entropies:\"\n",
    "ppEandT(answer11[:10])\n",
    "print \"Worst 10 entropies:\"\n",
    "ppEandT(answer11[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
