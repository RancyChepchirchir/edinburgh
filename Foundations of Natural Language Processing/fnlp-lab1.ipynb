{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in 2009-Obama.txt: 2726\n",
      "Total distinct words in 2009-Obama.txt: 900\n"
     ]
    }
   ],
   "source": [
    "__author__ = 's1667278'\n",
    "\n",
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Import the Presidential inaugural speeches corpus\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "# Import the gutenberg corpus\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Import NLTK's NgramModel module (for building language models)\n",
    "# It has been removed from standard NLTK, so we access it in a special package installation\n",
    "sys.path.extend(['/group/ltg/projects/fnlp', '/group/ltg/projects/fnlp/packages_2.6'])\n",
    "from nltkx import NgramModel\n",
    "\n",
    "\n",
    "#################### EXERCISE 1 ####################\n",
    "\n",
    "# Solution for exercise 1\n",
    "# Input: doc_name (string)\n",
    "# Output: total_words (int), total_distinct_words (int)\n",
    "def ex1(doc_name):\n",
    "    # Use the plaintext corpus reader to access a pre-tokenised list of words\n",
    "    # for the document specified in \"doc_name\"\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "\n",
    "    # Find the total number of words in the speech\n",
    "    total_words = len(doc_words)\n",
    "\n",
    "    # Find the total number of DISTINCT words in the speech\n",
    "    total_distinct_words = len(set([w.lower() for w in doc_words]))\n",
    "\n",
    "    # Return the word counts\n",
    "    return (total_words, total_distinct_words)\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 1\n",
    "speech_name = '2009-Obama.txt'\n",
    "(tokens,types) = ex1(speech_name)\n",
    "print \"Total words in %s: %s\"%(speech_name,tokens)\n",
    "print \"Total distinct words in %s: %s\"%(speech_name,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length for 2009-Obama.txt: 6.05555555556\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 2 ####################\n",
    "\n",
    "# Solution for exercise 2\n",
    "# Input: doc_name (string)\n",
    "# Output: avg_word_length (float)\n",
    "def ex2(doc_name):\n",
    "\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "\n",
    "    # Construct a list that contains the word lengths for each DISTINCT word in the document\n",
    "    distinct_word_lengths = [len(w) for w in set([w.lower() for w in doc_words])]\n",
    "\n",
    "    # Find the average word length\n",
    "    avg_word_length = np.mean(distinct_word_lengths)\n",
    "\n",
    "    # Return the average word length of the document\n",
    "    return avg_word_length\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 2\n",
    "speech_name = '2009-Obama.txt'\n",
    "result2 = ex2(speech_name)\n",
    "print \"Average word length for %s: %s\"%(speech_name,result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words for Obama's 2009 speech:\n",
      "[(u'the', 135), (u'and', 111), (u'of', 82), (u'to', 70), (u'our', 67), (u'we', 62), (u'that', 49), (u'a', 47), (u'is', 36), (u'in', 25), (u'this', 24), (u'for', 23), (u'us', 23), (u'are', 22), (u'but', 20), (u'will', 19), (u'it', 19), (u'they', 17), (u'on', 17), (u'have', 16), (u'not', 16), (u'who', 14), (u'has', 14), (u'you', 14), (u'can', 13), (u'with', 13), (u'nation', 12), (u's', 12), (u'be', 12), (u'new', 11), (u'those', 11), (u'or', 11), (u'as', 11), (u'their', 10), (u'america', 10), (u'all', 9), (u'so', 9), (u'these', 9), (u'must', 8), (u'every', 8), (u'what', 8), (u'its', 8), (u'than', 8), (u'been', 8), (u'because', 8), (u'by', 8), (u'at', 8), (u'let', 7), (u'do', 7), (u'less', 7)]\n",
      "Top 50 words for Washington's 1789 speech:\n",
      "[(u'the', 116), (u'of', 71), (u'to', 48), (u'and', 48), (u'which', 36), (u'in', 31), (u'i', 23), (u'be', 23), (u'my', 22), (u'by', 20), (u'that', 18), (u'with', 17), (u'on', 15), (u'a', 14), (u'as', 14), (u'have', 12), (u'for', 12), (u'it', 11), (u'this', 10), (u'will', 10), (u'an', 10), (u'every', 9), (u'can', 9), (u'your', 9), (u'more', 8), (u'me', 8), (u'government', 8), (u'no', 8), (u'from', 7), (u'been', 7), (u'is', 7), (u'public', 6), (u'may', 6), (u'than', 6), (u'under', 5), (u'his', 5), (u'country', 5), (u'present', 5), (u'has', 5), (u'citizens', 5), (u'or', 5), (u'you', 5), (u'united', 4), (u'its', 4), (u'not', 4), (u'since', 4), (u'one', 4), (u'duty', 4), (u'myself', 4), (u'being', 4)]\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 3 ####################\n",
    "\n",
    "# Solution for exercise 3\n",
    "# Input: doc_name (string), x (int)\n",
    "# Output: top_words (list)\n",
    "def ex3(doc_name, x):\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "\n",
    "    # Construct a frequency distribution over the lowercased words in the document\n",
    "    fd_doc_words = nltk.FreqDist(w.lower() for w in doc_words if w.isalnum())\n",
    "\n",
    "    # Find the top x most frequently used words in the document\n",
    "    top_words = fd_doc_words.most_common(50)\n",
    "\n",
    "    # Return the top x most frequently used words\n",
    "    return top_words\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 3\n",
    "print \"Top 50 words for Obama's 2009 speech:\"\n",
    "result3a = ex3('2009-Obama.txt', 50)\n",
    "print result3a\n",
    "print \"Top 50 words for Washington's 1789 speech:\"\n",
    "result3b = ex3('1789-Washington.txt', 50)\n",
    "print result3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense and Sensibility bigram language model built\n"
     ]
    }
   ],
   "source": [
    "#################### EXERCISE 4 ####################\n",
    "\n",
    "# Solution for exercise 4\n",
    "# Input: doc_name (string), n (int)\n",
    "# Output: lm (NgramModel language model)\n",
    "def ex4(doc_name, n):\n",
    "    # Construct a list of lowercase words from the document\n",
    "    words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Build the language model using the nltk.MLEProbDist estimator\n",
    "    lm = NgramModel(n, words)\n",
    "\n",
    "    # Return the language model (we'll use it in exercise 5)\n",
    "    return lm\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 4\n",
    "result4 = ex4('austen-sense.txt',2)\n",
    "print \"Sense and Sensibility bigram language model built\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################### EXERCISE 5 ####################\n",
    "\n",
    "# Solution for exercise 5\n",
    "# Input: lm (NgramModel language model, from exercise 4), word (string), context (list)\n",
    "# Output: p (float)\n",
    "def ex5(lm,word,context):\n",
    "    # Compute the probability for the word given the context\n",
    "    p = lm.prob(word,context)\n",
    "\n",
    "    # Return the probability\n",
    "    return p\n",
    "\n",
    "\n",
    "## Uncomment to test exercise 5\n",
    "result5a = ex5(result4,'for',['reason'])\n",
    "print \"Probability of \\'reason\\' followed by \\'for\\': %s\"%result5a\n",
    "result5b = ex5(result4,'end',['the'])\n",
    "print \"Probability of \\'the\\' followed by \\'end\\': %s\"%result5b\n",
    "result5c = ex5(result4,'the',['end'])\n",
    "print \"Probability of \\'end\\' followed by \\'the\\': %s\"%result5c\n",
    "\n",
    "## Uncomment to test exercise 6\n",
    "result6 = ex5(result4,'the',['end'],True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
